CogAgent: A Visual Language Model for GUI Agents
Wenyi Hong1* Weihan Wang1* Qingsong Lv2
Jiazheng Xu1* Wenmeng Yu2
Junhui Ji2 Yan Wang2 Zihan Wang1* Yuxuan Zhang2* Juanzi Li1
Bin Xu1 Yuxiao Dong1 Ming Ding2†
Jie Tang1†
1Tsinghua University 2Zhipu AI
{hwy22@mails, jietang@}.tsinghua.edu.cn, ming.ding@zhipuai.cn
Abstract
People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g.,
computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks
like writing emails, but struggle to understand and interact
with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-
billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both
low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120×1120, enabling
it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of
the art on five text-rich and four general VQA benchmarks,
including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA,
infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using
only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and
Android GUI navigation tasks—Mind2Web and AITW, advancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM.
1. Introduction
Autonomous agents in the digital world are ideal assistants
that many modern people dream of. Picture this scenario:
You type in a task description, then relax and enjoy a cup
of coffee while watching tasks like booking tickets online,
conducting web searches, managing files, and creating PowerPoint presentations get completed automatically.
Recently, the emergence of agents based on large language models (LLMs) is bringing us closer to this dream.
For example, AutoGPT [33], a 150,000-star open-source
project, leverages ChatGPT [29] to integrate language un-
*Work was done when interned at Zhipu AI.
†Corresponding authors
derstanding with pre-defined actions like Google searches
and local file operations. Researchers are also starting to
develop agent-oriented LLMs [7, 42]. However, the potential of purely language-based agents is quite limited in realworld scenarios, as most applications interact with humans
through Graphical User Interfaces (GUIs), which are characterized by the following perspectives:
• Standard APIs for interaction are often lacking.
• Important information including icons, images, diagrams, and spatial relations are difficult to directly convey in words.
• Even in text-rendered GUIs like web pages, elements
like canvas and iframe cannot be parsed to grasp their
functionality via HTML.
Agents based on visual language models (VLMs) have
the potential to overcome these limitations. Instead of relying exclusively on textual inputs such as HTML [28] or
OCR results [31], VLM-based agents directly perceive visual GUI signals. Since GUIs are designed for human users,
VLM-based agents can perform as effectively as humans,
as long as the VLMs match human-level vision understanding. In addition, VLMs are also capable of skills such as
extremely fast reading and programming that are usually
beyond the reach of most human users, extending the potential of VLM-based agents. A few prior studies utilized
visual features merely as auxiliaries in specific scenarios.
e.g. WebShop [39] which employs visual features primarily for object recognition purposes. With the rapid development of VLM, can we naturally achieve universality on
GUIs by relying solely on visual inputs?
In this work, we present CogAgent, a visual language
foundation model specializing in GUI understanding and
planning while maintaining a strong ability for general
cross-modality tasks. By building upon CogVLM [38]—a
recent open-source VLM, CogAgent tackles the following
challenges for building GUI agents:
1
arXiv:2312.08914v2 [cs.CV] 21 Dec 2023
Figure 1. Samples of visual agents generated by CogAgent. More samples are demonstrated in the Appendix.
2
• Training Data. Most current VLMs are pre-trained
on datasets like LAION [32], consisting of natural images on the Web. However, we notice that the GUI
images share a different distribution from natural images. We thus construct a large-scale annotated dataset
about GUIs and OCR for continual pre-training.
• High-Resolution vs. Compute. In GUIs, tiny icons
and text are ubiquitous, and it is hard to recognize
them in commonly-used 224 × 224 resolution. However, increasing the resolution of input images results
in significantly long sequence length in language models. For example, a 1120 × 1120 image corresponds
to a sequence of 6400 tokens if the patch size is 14,
demanding excessive training and inference compute.
To address this, we design a cross-attention branch
that allows for a trade-off between the resolution and
the hidden size within a proper computation budget.
Specifically, we propose to combine the original large
ViT [12] (4.4B parameters) used in CogVLM [38] and
a new small high-resolution cross-module (with image
encoder of 0.30B parameters) to jointly model visual
features.
Our experiments show that:
• CogAgent tops popular GUI understanding and
decision-making benchmarks, including AITW [31]
and Mind2Web [10]. To the best of our knowledge,
this is the first time that a generalist VLM can outperform LLM-based methods with extracted structured
text.
• Though CogAgent focuses on GUIs, it achieves
state-of-the-art generalist performance on nine visual
question-answering benchmarks including VQAv2 [1],
OK-VQA [23], TextVQA [34], ST-VQA [4],
ChartQA [24], infoVQA [26], DocVQA [25],
MM-Vet [41], and POPE [19].
• The separated design of high- and low-resolution
branches in CogAgent significantly lows the compute
cost for consuming high-resolution images, e.g., the
number of the floating-point operations (FLOPs) for
CogAgent-18B with 1120 × 1120 inputs is less than
half that of CogVLM-17B with its default 490 × 490
inputs.
CogAgent is open-sourced at https://github.
com/THUDM/CogVLM. It represents an effort to promote
the future research and application of AI agents, facilitated
by advanced VLMs.
2. Method
In this section, we will first introduce the architecture of CogAgent, especially the novel high-resolution cross-module,
and then illustrate the process of pre-training and alignment
in detail.
2.1. Architecture
The architecture of CogAgent is depicted in Fig. 2. We
build our model based on a pre-trained VLM (on the right
side of the image), and propose to add a cross-attention
module to process high-resolution input (on the left side
of the image). As our base VLM, We select CogVLM17B [38], an open-sourced and state-of-the-art large visonlanguage model. Specifically, We employ EVA2-CLIPE [35] as the encoder for low-resolution images (224×224
pixels), complemented by an MLP adapter that maps its
output into the feature space of the visual-language decoder. The decoder, a pre-trained language model, is enhanced with a visual expert module introduced by Wang
et al. [38] to facilitate a deep fusion of visual and language
features. The decoder processes a combined input of the
low-resolution image feature sequence and text feature sequence, and autoregressively outputs the target text.
Similar to most VLMs, the original CogVLM can only
accommodate images of relatively low resolution (224 or
490), which hardly meets the demands of GUI where the
screen resolution of computers or smartphones is typically
720p (1280 × 720 pixels) or higher. It is a common problem among VLMs, e.g. LLaVA [21] and PALI-X [8] are
pre-trained at a low resolution of 224 × 224 on the general
domain. The primary reason is that high-resolution image
brings prohibitive time and memory overhead: VLMs usually concatenate text and image feature sequence as input
to the decoder, thus the overhead of self-attention module
is quadratic to the number of visual tokens (patches), which
is quadratic to the image’s side length. There are some initial attempts to reduce costs for high-resolution images. For
instance, Qwen-VL [2] proposes a position-aware visionlanguage adapter to compress image features, but only reduces sequence length by four and has a maximum resolution of 448 × 448. Kosmos-2.5 [22] adopts a Perceiver
Resampler module to reduce the length of the image sequence. However, the resampled sequence is still long for
self-attention in the large visual-language decoder (2,048
tokens), and can only be applied to restricted text recognition tasks.
Therefore, we propose a novel high-resolution crossmodule as a potent complement to the existing structure
for enhancing understanding at high resolutions, which not
only maintains efficiency confronting high-resolution images, but also offers flexible adaptability to a variety of
visual-language model architectures.
2.2. High-Resolution Cross-Module
The structural design of high-resolution cross-module is
mainly based on the following observations:
3
Input Image ( ) Input Image ( ) Input Text
Word Embedding
i-th layer
L-th layer
…
concat
High-Resolution Cross-Module Original VLM [Target Text]
MLP Adapter
Task: How can I find an
apartment that offers free
Wi-Fi?
Plan: 1.Locate and select
the price filter option.
2.Select the 'Free Wi-Fi'
option. 3. Apply the filters
to update the search
results, and choose one
satisfying apartment.
Action: Move the cursor
to the 'Price filter' on the
left sidebar where it says
'Your previous filters’, and
click on the 'Free Wi-Fi'
checkbox.
[ High-resolution image feature ] [Low-resolution image feature] [Text feature]
downsample
High-resolution
Image Encoder
(light-weight)
Low-resolution
Image Encoder
Visual Language Decoder
(hidden size = 4096)
1st layer
Cross-Attention
(hidden size = 1024)
cross-attn
cross-attn
…
Figure 2. Model architecture of CogAgent. We adopt CogVLM as
the original VLM.
1. At a modest resolution such as 224 × 224, images
can depict most objects and layouts effectively, yet
the resolution falls short in rendering text with clarity. Hence, our new high-resolution module should
emphasize text-related features, which are vital for understanding GUIs.
2. While pre-trained VLMs in general domain often
need large hidden sizes (e.g. 4,096 in PALI-X and
CogVLM, 5,120 in LLaVA), VLMs tailored for textcentered tasks like document OCR require smaller hidden sizes to achieve satisfying performance (e.g. 1,536
in Kosmos-2.5 and Pix2Struct [16]). This suggests that
text-related features can be effectively captured using
smaller hidden sizes.
As shown in Fig. 2, the high-resolution cross-module
acts as a new branch for higher-resolution input, which
accepts images of size 1120 × 1120 pixels in our implementation. Different from the original low-resolution
input branch, the high-resolution cross-module adopts a
much smaller pre-trained vision encoder (visual encoder of
EVA2-CLIP-L [35] in our implementation, 0.30B parameters), and uses cross-attention of a small hidden size to fuse
high-resolution image features with every layer of VLLM
decoder, thus reducing the computational cost. To be concrete, for an input image, it is resized to 1120 × 1120 and
224×224 and fed into the high-resolution cross-module and
the low-resolution branch respectively, then encoded into
image feature sequences Xhi and Xlo with two distinct-sized
image encoders in parallel. The visual language decoder retains its original computations, while the only change is to
integrate a cross-attention between Xhi and hidden states in
every decoder layer.
Formally, suppose that the input hidden states of
the i-th attention layer in the decoder are Xini ∈
R
B×(LIlo+LT )×Ddec , and the output hidden states of crossmodule’s image encoder are Xhi ∈ R
B×(LIhi )×Dhi , where B
is the batch size, LIlo , LIhi and LT are the lengths of the
low-resolution image, high-resolution image and text sequences, Ddec and Dhi is the hidden size of the decoder and
high-resolution encoder’s output respectively. Each layer’s
attention procedure can be formulated as
X′
i = MSA(layernorm(Xini
)) + Xini
, (1)
Xouti = MCA(layernorm(X′
i
), Xhi) + X′
i
, (2)
where MSA and MCA represent multi-head self-attention
with visual expert and multi-head cross-attention, while
X′
i
and Xouti
represent their respective output features
with the residual connection. To implement cross-attention
between them, we add learnable transformation matrices
Wi
Kcross
, Wi
Vcross
∈ R
Dhi×Dcross to get Ki
cross = XhiWi
Kcross
,
V
i
cross = XhiWi
Vcross
∈ R
LIhi×Dcross , and Wi
Qcross
∈ R
Ddec×Dcross
to get Qi
cross = X′
iWi
Qcross
∈ R
(LIlo+LT )×Dcross in every decoder layer. With the residual connection in Eq. 2, the crossattention with high-resolution images can be perceived as
a complement to the features of low-resolution images,
thereby effectively utilizing the previous pre-trained model
in low resolution.
Computational complexity. Let the number of attention
head be Hcross and Hdec in cross-attention and self-attention,
and the dimension of each head be dcross = Dcross/Hcross
and ddec = Ddec/Hdec. If using our high-resolution crossmodule, the computational complexity of attention is
Timproved = O

(LIlo + LT )LIhiHcrossdcross
+ (LIlo + LT )
2Hdecddec
.
(3)
Note that dcross and Hcross can be flexibly adjusted according to computational budget and model performance. If not
utilizing the high-resolution cross-module and directly substituting low-resolution images with high-resolution ones,
the computational complexity would be
Toriginal = O

(LIhi + LT )
2Hdecddec
. (4)
In our implementation, dcross = 32, Hcross = 32, and
we inherits ddec = 128, Hdec = 32 from CogVLM-17B.
Both high- and low-resolution encoders patchify images
with 14 × 14-pixel patches, thus LIhi = 6400, LIlo = 256.
Our method leads to at least LIhi+LT
LIlo+LT
=
6400+LT
256+LT
× acceleration which is a stringent lower bound (refer to Appendix
for detailed derivation), and reduces memory overhead at
the same time.

2.3. Pre-training
To enhance the model’s ability to comprehend highresolution images and adapt it for GUI application scenarios, we focus our pre-training efforts on the following aspects: the capability to recognize texts of various sizes, orientations, and fonts in high-resolution images, the grounding ability of text and objects in the image, and a specialized understanding capability for GUI imagery such as web
page. We divide our pre-train data into three parts based on
the aforementioned aspects, with samples in the Appendix.
All the pre-training data are derived from publicly available
datasets. The construction methods are detailed below.
Text recognition. Our data includes (1) Synthetic renderings with text from language pre-training dataset (80M).
This is similar to the Synthetic Document Generator in Kim
et al. [15], with text of varying font, size, color and orientation, and diverse image background from LAION-2B [32].
(2) Optical Character Recognition (OCR) of natural images (18M). We collect natural images from COYO [6] and
LAION-2B [32] and employ Paddle-OCR [13] to extract the
texts and their bounding boxes, and filter out images with
no text boxes. Paddle-OCR may introduce some errors,
which can be ameliorated through integration with other
pre-training datasets and subsequent fine-tuning processes.
(3) Academic documents (9M). We follow Nougat [5] to
construct image-text pairs including text, formula and tables
from the source code (LaTeX) release on arXiv. For (1)(3),
we apply the same data augmentation as Nougat which includes erosion, gaussian noise, gaussian blur, image compression, and elastic transform, etc. For (2), we additionally
employed more aggressive rotation and flipping data augmentation techniques, thereby enhancing the model’s robustness in recognizing text.
Visual grounding. It is imperative for GUI agents to possess the capability to accurately comprehend and locate
diverse elements within images. Therefore, we incorporated a range of grounding data into pre-training. We follow CogVLM [38] to use a constructed visual grounding
dataset of 40M images with image-caption pairs sampled
from LAION-115M [18], which associate entities in the
caption with bounding boxes to indicate their positions.
The format of the bounding box is [[x0, y0, x1, y1]], where
(x0, y0) and (x1, y1) represent the coordinates of upper-left
and lower-right corners which are normalized to [000, 999].
If multiple objects are indicated by a single noun phrase,
their boxes are separated by semicolons in double square
brackets. We have also collected grounding data on web
page elements, which will be introduced in the next part.
GUI imagery. Our approach innovatively addresses the
scarcity and limited relevance of GUI images in datasets
like LAION and COYO, which predominantly feature natural images. GUI images, with their distinct elements such
as input fields, hyperlinks, icons, and unique layout characteristics, require specialized handling. To boost the model’s
capability in interpreting GUI imagery, we have conceptualized two pioneering GUI grounding tasks: (1) GUI Referring Expression Generation (REG) – where the model
is tasked with generating HTML code for DOM (Document Object Model) elements based on a specified area in a
screenshot, and (2) GUI Referring Expression Comprehension (REC) – which involves creating bounding boxes for
given DOM elements. To facilitate robust training in GUI
grounding, we have constructed the CCS400K (Common
Crawl Screenshot 400K) dataset. This extensive dataset is
formed by extracting URLs from the latest Common Crawl
data, followed by capturing 400,000 web page screenshots.
Alongside these screenshots, we compile all visible DOM
elements and their corresponding rendered boxes using
Playwright1
, supplementing the dataset with 140 million
REC and REG question-answer pairs. This rich dataset ensures comprehensive training and understanding of GUI elements. To mitigate the risk of overfitting, we employ a
diverse range of screen resolutions for rendering, selected
randomly from a list of commonly used resolutions across
various devices. Additionally, to prevent the HTML code
from becoming overly extensive and unwieldy, we perform
necessary data cleaning by omitting redundant attributes in
the DOM elements, following the method outlined in [16].
We also incorporate publicly available text-image
datasets including LAION-2B and COYO-700M (after removing the broken URLs, NSFW images, and images with
noisy captions and political bias) during pre-training.
We pre-train our CogAgent model for a total of 60,000
iterations with a batch size of 4,608 and a learning rate of
2e-5. We freeze all parameters except the newly added highresolution cross-module for the first 20,000 steps, resulting
in a total number of 646M (3.5%) trainable parameters, then
additionally unfreeze the visual expert in CogVLM for the
next 40,000 steps. We warm up with curriculum learning by
first training on easier text recognition (synthetic renderings
and OCR on natural images) and image captioning, then sequentially incorporating harder text recognition (academic
document), grounding data and web page data, as we observed that it leads to faster convergence and more stable
training in our preliminary experiments.
2.4. Multi-task Fine-tuning and Alignment
To enhance our model’s performance for diverse tasks and
ensure it aligns with free-form human instructions in the
GUI setting, we further fine-tune our model on a broad
range of tasks. We manually collected over two thousand
screenshots from mobile phones and computers, each annotated with screen elements, potential tasks, and methods of
operation in the question-answering format by human annotators (details illustrated in the Appendix). We also utilize
1https://playwright.dev
5
Mind2Web [10] and AITW [31], datasets focusing on web
and Android behaviors which comprise tasks, sequences of
actions and corresponding screenshots, and convert them
into a natural language question-and-answer format using
GPT-4. Besides, we incorporate multiple publicly available
visual question-answering (VQA) datasets encompassing a
variety of tasks into our alignment dataset. We unfreeze all
model parameters during this stage and train for 10k iterations with a batch size of 1024 and a learning rate of 2e-5.
3. Experiments
To evaluate the foundational capabilities and GUI-related
performance of our model, we conduct extensive experiments on a broad range of datasets. First, we conduct evaluations on eight VQA benchmarks, as well as MM-Vet [41]
and POPE [19], which validate our model’s enhanced ability in visual understanding, especially on those that are reliant on text recognition. Then we evaluate our model on
Mind2Web and AITW datasets, as the representative of two
major GUI scenarios — computers and smartphones.
3.1. Foundational Visual Understanding
We first extensively evaluate CogAgent’s foundational visual understanding capability across eight VQA benchmarks, covering a wide range of visual scenes. The benchmarks can be divided into two categories: general VQA, including VQAv2 [1] and OK-VQA [23], and text-rich VQA,
including TextVQA [34], OCR-VQA [27], ST-VQA [4],
DocVQA [25], InfoVQA [26] and ChartQA [24]. The latter
category emphasizes the understanding of visually-situated
text, including documents, charts, photographs containing
text, etc. Contrary to models individually fine-tuned for optimal performance on each downstream task, our model is
fine-tuned collectively on all datasets simultaneously, yielding a single generalist model which is then evaluated across
all datasets. The goal of generalist evaluation is to better
mirror real-world situations of visual agents where typically
a single model is used, and to demonstrate the model’s versatility and robustness across tasks.
The results are presented in Tab. 1. For general VQA,
CogAgent achieves state-of-the-art generalist results on
both datasets. For text-rich VQA, CogAgent achieves
state-of-the-art results on 5 out of 6 benchmarks, significantly surpassing generalist competitors (TextVQA+8.0,
ChartQA+2.1, InfoVQA+2.3, DocVQA+16.2), even outperforming the task-specific state-of-the-art models on
TextVQA(+4.7), STVQA(+0.6) and DocVQA(+1.6). Notably, compared to the generalist results of CogVLM which
CogAgent is initially based on, CogAgent demonstrates
certain improvements on both general and Text-rich VQA
tasks, suggesting the efficacy of our proposed model architecture and training methods.
Furthermore, we conducted zero-shot tests of our model
on the challenging MM-Vet [41] and POPE [19] datasets,
both of which are instrumental in gauging the multi-modal
capabilities and the generalization performance in complex tasks including conversation question-answering, detailed descriptions, complex reasoning tasks. MM-Vet is
designed with six core tasks to assess multi-modal models’
proficiency in handling intricate assignments, and POPEadversarial models on their susceptibility to hallucinations.
Our experimental results, as detailed in Table 2, showcase that our model significantly outperforms other existing
models in both datasets. Notably, on the MM-Vet dataset,
our model achieved a remarkable score of 52.8, surpassing
the closest competitor, LLaVA-1.5, by a substantial margin
(+16.5). On the POPE-adversarial evaluation, our model attained a score of 85.9, demonstrating superior handling of
hallucinations compared to other models.
These results indicate CogAgent’s robust performance in
foundational visual understanding, especially in the interpretation of images with embedded text. With these core
competencies, the model can be feasibly applied to various
visual agent tasks across different GUI environments.
3.2. GUI Agent: Computer Interface
We evaluate CogAgent on Mind2Web, a dataset for web
agents that includes over 2,000 open-ended tasks collected
from 137 real-world websites across 31 domains. Each entry in the dataset comprises a high-level task description, a
sequence of actions, and webpage snapshots in a variety of
formats, including HTML and screenshots. Given task description, current webpage snapshot and previous actions as
inputs, agents are expected to predict the subsequent action.
We follow the setting of Deng et al. [10] in our experiments,
and report step success rate (step SR) metric. Further details
are attached in the Appendix.
Several language models were evaluated on this benchmark. For instance, AgentTuning [42] and MindAct [10]
evaluated Llama2-70B and Flan-T5-XL in a fine-tuned setting, and GPT-3.5 and GPT-4 in a in-context learning setting. However, limited by the input modality of language models, these models could only use heavily cleansed
HTML as the representation of screen inputs. To the best of
our knowledge, no visually-based web agents have been experimented with on this benchmark.
We fine-tune our model on the train set and evaluate on three out-of-domain subsets, i.e. cross-website,
cross-domain, and cross-task. We additionally fine-tune
LLaMA2-7B and LLaMA2-70B as the baseline of finetuned LLMs, and adopt the same HTML cleansing process
as Deng et al. [10] to construct HTML input. The results
are presented in Sec. 3.2. Compared to other methods, our
approach achieved significant performance improvements
across all three subsets, surpassing LLaMA2-70B, which
6
Method General VQA Text-rich VQA
VQAv2 OKVQA OCRVQA TextVQA STVQA ChartQA InfoVQA DocVQA
task-specific fine-tuning models
Pix2Struct [16] - - - - - 58.6 40.0 76.6
BLIP-2 [18] 82.2 59.3 72.7 - - - - -
PALI-X-55B [8] 86.0 66.1 75.0 71.4 79.9 70.9 49.2 80.0
CogVLMtask-specific [38] 84.7 64.7 74.5 69.7 - - - -
generalist models
UReader [40] - 57.6 - - - 59.3 42.2 65.4
Qwen-VL [2] 79.5 58.6 75.7 63.8 - 65.7 - 65.1
Qwen-VL-chat [2] 78.2 56.6 70.5 61.5 - 66.3 - 62.6
Llava-1.5 [20] 80.0 - - 61.5 - - - -
Fuyu-8B [3] 74.2 60.6 - - - - - -
CogVLMgeneralist [38] 83.4 58.9 74.1 68.1 - - - -
CogAgent (Ours) 83.7 61.2 75.0 76.1 80.5 68.4 44.5 81.6
Table 1. Performance on Visual Question Answering benchmarks. Bold text indicates the best score among the generalist category, and
underlined text represents the best score across both generalist and task-specific categories.
is nearly 4× the scale of CogAgent, by 11.6%, 4.7%, and
6.6%, respectively. This reflects not only the capability of
our model but also the advantages of employing a visual
agent in computer GUI scenarios.
3.3. GUI Agent: Smartphone Interface
To evaluate our model on diverse smartphone interfaces and
tasks, we utilize Android in the Wild (AITW) dataset [31] ,
a large-scale dataset for Android device agents. It comprises
715k operation episodes, covering 30k distinct task instructions, four Android versions, and eight device types featuring varying screen resolutions. Each episode in the dataset
consists of a goal described in natural language, followed
by a sequence of actions and corresponding screenshots.
The training target is to predict the next action based on the
given goal, historical actions, and the screenshot. AITW
considers a wide range of action types, including tapping,
swiping, typing, going home, going back, entering, etc. For
each action, models are required to predict the exact action
type; for tap, swipe and type, models are further required
Method LLM MM-Vet POPEadv
BLIP-2 [18] Vicuna-13B 22.4 -
Otter [17] MPT-7B 24.7 -
MiniGPT4 [44] Vicuna-13B 24.4 70.4
InstructBLIP [9] Vicuna-13B 25.6 77.3
LLaVA [21] LLaMA2-7B 28.1 66.3
LLaMA-Adapter v2 [14] LLaMA-7B 31.4 -
DreamLLM [11] Vicuna-7B 35.9 76.5
LLaVA-1.5 [20] Vicuna-13B 36.3 84.5
Emu [36] LLaMA-13B 36.3 -
CogAgent (Ours) Vicuna-7B 52.8 85.9
Table 2. Evaluation of CogAgent on conversational style QA
and hallucination assessment. Regarding the POPE dataset, we
use its adversarial subset for this evaluation.
Method cross-task cross-website cross-domain overall
Representations of screen inputs: HTML
GPT-3.5[29](few-shot) 18.6 17.4 16.2 17.4
GPT-4[30]
†
(few-shot) 36.2 30.1 26.4 30.9
Flan-T5XL [10] 52.0 38.9 39.6 43.5
LLaMA2-7B[37] 52.7 47.1 50.3 50.1
LLaMA2-70B[37] 55.8 51.6 55.7 54.4
Representations of screen inputs: Image
CogAgent (Ours) 62.3 54.0 59.4 58.2
Table 3. Performance on Mind2Web. † denotes element selection from top-10 element candidates, others from top-50, following Deng et al. [10]. Results for GPT-3.5 and GPT-4 are from
Deng et al. [10].
Method GoogleApp Install WebShop General Single Overall
Representations of screen inputs: textual description (OCR+icon)
GPT-3.5[29](few-shot) 10.47 4.38 8.42 5.93 9.39 7.72
LLaMA2-7B[37]
† 30.99 35.18 19.92 28.56 27.35 28.40
Representations of screen inputs: image
Auto-UIunified[43] 71.37 76.89 70.26 68.24 84.58 74.27
CogAgent (Ours) 74.95 78.86 71.73 65.38 93.49 76.88
Table 4. Performance on Android in the Wild (AITW) dataset.
† represents models individually fine-tuned on each subset, while
others are unified models across all subsets. The results of
LLaMA2 and GPT-3.5 are from Zhan and Zhang [43].
to predict the position, direction, and content to be typed,
respectively.
We conduct comparisons with two kinds of baselines:
language models using the textual description of UI elements provided by the original dataset (text OCR and icon)
as the representations of screen inputs2
, and visual-language
2Some Android applications may have View Hierarchy which is more
friendly to language-based agents, but most of them tend to be poor quality
or missing altogether. Therefore, as a large-scale, general-purpose dataset,
AITW retained the results of OCR detection and icon detection as textual
7
models using images as the screen inputs. We simultaneously fine-tuned on all the subsets, yielding a unified model
which is then evaluated on all test sets. As the GoogleApps
subset is 10-100 times larger than other subsets, we downsample it to 10% to avoid data imbalance.
Results are shown in Tab. 4. CogAgent achieves stateof-the-art performance compared to all previous methods.
In comparison to language-based methods, our model surpasses both baselines by a large margin. In comparison to
the visual-language baseline, Auto-UI, our model achieves
+2.61 improvements in the overall performance. In instances of inaccuracies, we randomly sample hundreds of
cases, and upon reassessment, more than 40% are determined to be correct (refer to the appendix for details). This
diversity arises from the multiple valid pathways inherent in
mobile interactions, resulting in a range of responses.
4. Ablation Study
To thoroughly comprehend the impact of various components in the methodology, we conduct ablation studies on
two aspects, model architecture and training data. The evaluation is conducted on diverse datasets, including multiple
VQA datasets (STVQA, OCRVQA, DocVQA) and a web
agent dataset (Mind2Web). For VQA datasets, we finetune the model on four datasets together for 3,000 iters with
a batch size of 1,280, and report the generalist score; for
Mind2Web, models are fine-tuned for 2,400 iters with a
batch size of 128 and use top-10 setting. Training iterations
are fewer than those in the main experiment, aiming to control variables within the constraints of a limited budget.
4.1. Model Architecture
To ascertain the efficacy of the high-resolution crossmodule, we compare it with directly increasing the resolution using the original model architecture of CogVLM, and
ablate on two perspectives: computational efficiency and
model performance.
To measure computational overhead, we use floating
point operations (FLOPs) as the metric, and conduct experiments on multiple resolutions including 224, 490, 756, and
1120. From Fig. 3 we can see that, as the image resolution
increases, models that use a high-resolution cross-module
experience only a modest rise in computational overhead,
demonstrating an almost linear relationship with the number of image patches. In contrast, using the original model
structure, i.e. CogVLM, leads to a significant increase in
the number of FLOPs at higher resolutions. Its FLOPs can
even be more than 10 times higher compared to employing
a cross-module at a resolution of 1120, which is the resolution utilized by CogAgent.
representations of screenshots.
256 490 756 1120
Resolution
20
40
60
80
100
120
140
TFLOPs
8.8 10.1 12.6
29.1
66.3
143.2
7.8
with cross-module
original architecture
Figure 3. Comparison of FLOPs during forward propagation for
different model architectures and resolutions.
We further compare the model performance in Tab. 5,
which indicates that models with high-resolution crossmodule at the resolution of 756 require only 1/2 of the computational resources used by the original structure at the
resolution of 490, while delivering significantly better performance. Additionally, the high-resolution cross-module
allows for further increasing models’ acceptable resolution
within a limited computational budget, thereby yielding additional performance improvements.
high-res base cross STVQA OCRVQA DocVQA Mind2Web training TFLOPs module res res time/it (s)
% 224 — 48.0 70.2 28.6 34.6 2.36 7.77
% 490 — 68.1 74.5 57.6 40.7 6.43 29.14
! 224 756 73.6 74.2 62.3 40.7 3.57 10.08
! 224 1120 78.2 75.9 74.1 41.4 5.17 12.56
Table 5. Ablation study on model architecture. Training time is
evaluated on A800 with the batch size of 8. Models are pre-trained
with Caption+OCR data.
4.2. Pre-train Data
pre-train data base res cross res STVQA OCRVQA DocVQA Mind2Web
Cap 490 — 68.1 74.5 57.6 38.6
Cap+OCR 490 — 72.5 75.0 59.8 40.7
Cap+OCR 224 1120 78.2 75.9 74.1 41.4
All 224 1120 79.4 75.6 76.4 54.2
Table 6. Ablation study on pre-train data with sequentially added
image captioning, OCR and other pre-train data.
We further conduct an ablation study on pre-training
data, which is an integral part of training visual agents.
Building upon the image-caption data commonly used in
visual-language training, we sequentially add OCR data
(denoted as Cap+OCR), as well as GUI and grounding data
(denoted as All). The results in Tab. 6 indicate that each
part of data broadly contributes to enhanced performance.
Notably, web and grounding data have a significant impact
on the Mind2Web dataset, underscoring the importance of
8
constructing domain-specific pre-train data in the training
of GUI agents.
5. Conclusion
We introduce CogAgent, a VLM-based GUI agent with enhanced pre-train data construction and efficient architecture
for high-resolution input. CogAgent achieves state-of-theart performance on a wide range of VQA and GUI benchmarks, and will be open-sourced.
CogAgent is an initial exploration of VLM-based GUI
agent, and still has some shortcomings, e.g. imprecise output coordinates and incapability of processing multiple images, necessitating further research.
Acknowledgments
We thank Xiaohan Zhang from Zhipu AI for managing the
data annotation team, and Zhao Xue, Aohan Zeng, Yifan
An, Chenxu Guo from Zhipu AI and Tsinghua for data man

CogAgent: A Visual Language Model for GUI Agents
Appendix
1. Details of Training Configurations
We report the detailed training settings of CogAgent in Table 7, and model configurations of CogAgent in Table 8.
Configurations Pre-train Multi-task
Total steps 60, 000 10, 000
Warmup steps 500 500
Batch size 4, 608 1, 024
Learning rate 2 × 10−5
Learning rate decay Cosine
Weight decay 0.05
Dropout ratio 0.1
Adam ϵ 1 × 10−5
Adam β (0.9, 0.95)
Table 7. Training settings of pre-training and multi-task finetuning.
VLM decoder
Architecture Vicuna-1.5-7B + visual expert
Layers 32
Hidden size 4, 096
Attention heads 32
Low-resolution visual encoder
Architecture EVA2-CLIP-E
Input resolution 224 × 224
Patch size 14 × 14
High-resolution visual encoder
Visual encoder EVA2-CLIP-L
Input resolution 1120 × 1120
Patch size 14 × 14
Cross Attention
Hidden size 1, 024
Attention heads 32
Table 8. Model configurations of CogAgent.
2. Details of Evaluation Datasets
In this section, we will provide a detailed overview of the
datasets used in our evaluations .
2.1. General VQA
• VQAv2 [1]. VQAv2 is designed for visual question answering with natural images, covering a wide range of
question types including yes/no, numerical counting,
and more open-ended inquiries. The dataset comprised
of a collection exceeding 200,000 images, paired with
more than 1.1 million questions. Each question is
paired with 10 answers annotated by different annotators.
• OK-VQA [23]. OK-VQA (Outside Knowledge Visual
Question Answering) dataset is constructed to evaluate
visual question-answering skills that require external
knowledge. Models need to combine image content
and common sense to answer questions. The dataset
includes 14,055 open-ended questions, each accompanied by 5 ground truth answers.
• MM-Vet [41]. MM-Vet is designed to evaluate the
overall capability of generalist visual language models
in a zero-shot manner. It integrates 6 core VL capabilities to solve complex tasks (including recognition,
OCR, knowledge, language generation, spatial awareness, and math), and explores 16 distinct integrations
resulting from the combination of these capabilities.
As for evaluation metrics, it utilizes a language modelbased evaluator tailored for open-ended responses.
• POPE [19]. POPE (Polling-based Object Probing
Evaluation) is a dataset constructed to assess the object
hallucination problem in large visual language models. It employs a polling-based object probing method,
transforming hallucination assessment into a binary
classification challenge. This is achieved by prompting
large vision-language models (LVLMs) with straightforward Yes-or-No queries regarding the objects in
question (for example, ”Is there a car in the image?”).
Our evaluation is conducted under the dataset’s most
challenging setting: the adversarial setting.
2.2. Text-rich VQA
• OCR-VQA [27]. OCR-VQA dataset comprises
207,572 images of book covers, paired with more than
1 million question-answer pairs. The questions inquire about book information including title, edition,
year, author, and genre of the book, which requires text
recognition and comprehension abilities.
• TextVQA [34]. TextVQA is a benchmark of visual
reasoning based on text in images. Models need to
11
Task Dataset Description Split Metrics
General VQA
VQAv2 VQA on natural images. test-dev VQA Score(↑)
OK-VQA VQA on natural images requiring outside knowledge. val VQA Score (↑)
MM-Vet Conversational style VQA on integrated capabilities. test GPT-4 score(↑)
POPE VQA for hallucination assessment. The adversarial setting is used. test F1 score(↑)
Text-rich VQA
OCR-VQA VQA on images of book covers. test EM (↑)
TextVQA VQA on natural images containing text. test VQA Score (↑)
ST-VQA VQA on natural images requiring textual understanding. test ANLS (↑)
ChartQA VQA about charts with visual and logical reasoning. test VQA Score (↑)
InfoVQA VQA on infographics. test ANLS (↑)
DocVQA VQA on document images. test ANLS (↑)
GUI Agent Mind2Web Web behavior prediction given snapshots and historical actions. test step SR (↑)
AITW Android behavior prediction given snapshots and historical actions. test Matching Score (↑)
Table 9. Summary of the evaluation benchmarks.
incorporate the textual information in the images and
reason over it to answer TextVQA questions. It comprises a total of 28,408 images and 45,336 questions.
• ST-VQA [4]. ST-VQA is designed to emphasize the
significance of exploiting the semantic information
present within images in textual form during the VQA
process. It comprises tasks of diverse difficulties, for
which recognizing the scene text and performing necessary reasoning is required to generate the answer.
The dataset comprises 23,038 images sourced from
multiple public datasets and 31,791 question-answer
pairs.
• ChartQA [24]. ChartQA is a benchmark of questionanswering about logical and visual reasoning on
charts. It consists of 20,882 charts curated from four
different online sources, 9,608 manual written questions, as well as 23,111 questions automatically generated with T5 according to human-written chart summaries.
• InfographicVQA(InfoVQA) [26]. The task of InfoVQA is to answer questions centering on a given infographic image. The answers to most questions can
be extracted from the given documents, while the answers to a small percentage of questions are not extractive. There are 5K Images collected from the Internet
and 30K manually annotated questions in the dataset.
• DocVQA [25]. DocVQA focuses on questionanswering given a document image. The answer for
questions is often a span of text from the given documents. There are 12K images and 50K manually annotated questions in the datasets.
2.3. GUI Agent
CogAgent is evaluated on two GUI agent datasets,
Mind2Web and Android in the Wild (AITW), corresponding to computer agent and smartphone agent respectively.
• Mind2Web [10]. Mind2Web is designed to develop
and evaluate web agents capable of executing intricate tasks on various websites based on language directions. While existing datasets for web agents commonly rely on simulated or overly simplified web data,
Mind2Web utilizes real-world websites and is annotated by human annotators. It gathers data from 137
websites covering 31 domains, and collects over 2,000
open-ended tasks, each accompanied by a crowdsourced action sequence.
In mind2web, evaluated agents are asked to accomplish a designated task on a chosen website by performing a sequence of actions. Each instance (i.e. a
specific task) in Mind2Web contains a task description,
action sequence, and webpage snapshots. Each action
in the sequence is a (Target element, Operation) pair,
and Operation includes Click, Type (with additional
value), and Select (with additional value). Each action
is paired with the concurrent webpage snapshots in a
variety of formats including raw HTML code, DOM
tree, screenshot, etc. As for CogAgent, we choose
screenshot images as the input representation of websites; as for other language-based agents, HTML is
chosen as the input representation.
Following Deng et al. [10] and Zeng et al. [42], we formalize the problem as: first choose the target webpage
element among top-k (k=10 or 50) candidates, then
predict specific operations. The top-k candidates are
provided by the candidate generation model in Deng
et al. [10]. Step success rate (step SR) is reported on 3
out-of-domain test sets (cross-website, cross-domain,
12
cross-task) as metric. Only predictions with the same
target element and operation as the ground truth are
regarded as correct.
• Android in the Wild (AITW) [31]. AITW is constructed to develop and evaluate Android devicecontrol systems that are capable of understanding and
acting upon human natural language instructions by directly manipulating the device’s user interface. This
dataset significantly surpasses others in its category in
terms of size, encompassing 715k episodes across 30k
distinct instructions, and covering four Android versions (v10–13). It also includes eight types of devices,
ranging from Pixel 2 XL to Pixel 6, each with different screen resolutions. AITW consists of five subsets:
GoogleApps, Install, WebShopping, General, and Single. The subsets have distinct tasks, while are in the
same data format. Each episode (i.e. action sequence)
comprises three components: a goal instruction provided in natural language, a user action sequence, and
a corresponding screenshots sequence.
As for screenshots, AITW only provides screenshot
images and does not provide tree-based representations of UI. This is because a large portion of them
in Smartphone applications are of low quality or even
do not exist, and adopting the tree-based representation would strongly limit agents’ applications. For visual agents, screenshots are provided to the agents in
image format; for language-model-based agents evaluated by Zhan and Zhang [43], the textual representations of OCR and icons formatted in HTML syntax are
provided. As for actions, AITW considers a variety of
action types including tapping, swiping, typing, going
home, going back, entering, etc. For each action, models are required to predict the exact action type; for tap,
swipe and type, models are further required to predict
the position, direction, and content to be typed, respectively. The detailed standard for computing matching
scores is provided in Rawles et al. [31].
3. Derivation of Acceleration for HighResolution Cross-Module
Suppose that LIlo , LIhi and LT are the lengths of the lowresolution image, high-resolution image and text sequences.
Let Hcross, Hdec be the number of attention heads in crossattention and self-attention, and dcross, ddec be the dimension
of each attention head.
If using our high-resolution cross-module, the computational complexity of attention is
Timproved = O

(LIlo + LT )LIhiHcrossdcross
+ (LIlo + LT )
2Hdecddec
.
(5)
If not utilizing the high-resolution cross-module and
directly substituting low-resolution images with highresolution ones, the computational complexity would be
Toriginal = O

(LIhi + LT )
2Hdecddec
. (6)
The reduction factor of the computational complexity in
attention, Toriginal/Timproved, equals to
(LIhi + LT )
2Hdecddec
(LIlo + LT )LIhiHcrossdcross + (LIlo + LT )
2Hdecddec
(7)
=
LIhi + LT
LIlo + LT
(LIhi + LT )Hdecddec
LIhiHcrossdcross + (LIlo + LT )Hdecddec
(8)
=
LIhi + LT
LIlo + LT
(LIhi + LT )
Hdecddec
Hcrossdcross
LIhi + (LIlo + LT )
Hdecddec
Hcrossdcross
(9)
• Case 1: LIlo , LT ≪ LIhi .
Given that LIlo is much smaller than LIhi , when LT
also satisfies being much smaller than LIhi , both LIlo
LIhi
and LT
LIhi
become first-order small quantities. If conducting a 0-th order approximation for the complexity
reduction factor, we obtain:
Toriginal
Timproved
=
LIhi(1 + LT
LIhi
)
LIlo + LT
LIhi(1 + LT
LIhi
)
Hdecddec
Hcrossdcross
LIhi(1 + LIlo+LT
LIhi
Hdecddec
Hcrossdcross
)
(10)
=
LIhi(1 + LT
LIhi
)
LIlo + LT
(1 + LT
LIhi
)
Hdecddec
Hcrossdcross
1 + LIlo+LT
LIhi
Hdecddec
Hcrossdcross
(11)
≈
LIhi
LIlo + LT
(12)
• Case 2: Our implementation.
In our implementation, dcross = 32, Hcross = 32, and
we inherits ddec = 128, Hdec = 32 from CogVLM17B. Both high- and low-resolution encoders patchify
images with 14 × 14-pixel patches, thus LIhi = 6400,
LIlo = 256. Subsitute the values to Eq. (9), we obtain
Toriginal
Timproved
=
6400 + LT
256 + LT
4(6400 + LT )
6400 + 4(256 + LT )
(13)
>
6400 + LT
256 + LT
(14)
That is to say, our method leads to at least LIhi+LT
LIlo+LT
=
6400+LT
256+LT
× acceleration which is a stringent lower
bound. Taking pre-training as an example, with
LT typically below 512 in earlier stages, our highresolution cross-module can get an over 25× reduction
in computational cost.

4. Performance Analysis on AITW
Android in the Wild (AITW) dataset is a large-scale dataset
for Android device agents. Our analysis centers on instances where the CogAgent’s responses diverge from the
annotated ground truth, focusing on these discrepancies.
In our analysis, we conducted a sampling and manual
verification of cases where there was a discrepancy between
the model’s responses and the labeled answers. We sampled
instances and discovered that in 42% of these, the model’s
response actually represented an alternative correct method,
rather than an error. This phenomenon is common due to the
multiplicity of ways to accomplish a function on a smartphone. For example, many phones are equipped with both
Google’s app and a Google search bar, and searches can be
conducted using either. During the use of smartphones, human users may exhibit simultaneous needs for both in-depth
exploration and extensive browsing. For example, when
browsing news, CogAgent might identify relevant results on
the current page and select them immediately, while some
users may prefer to scroll through the entire page first. Furthermore, the data collection for annotations might involve
redundant or overly detailed steps, whereas CogAgent can
directly find the most efficient path. Fig. 4 illustrates these
typical examples of multiple correct answers.
Given the nature of smartphone usage, wherein accomplishing a task often involves multiple viable paths, it is advisable for future studies to explore a virtual environment
with an evaluation of the accuracy of task completion following a sequence of actions, aligning more closely with
real-world human usage scenarios.
CogAgent:
tap [[0.512, 0.403]].
User:
Set an alarm for 3pm.
Ground Truth:
tap [[0.285, 0.455]].
CogAgent:
tap [[0.672, 0.784]].
User:
Open a new Chrome incognito tab.
Ground Truth:
tap [[0.528, 0.870]].
CogAgent:
tap [[0.318, 0.246]].
User:
What's the news in Brazil?
Ground Truth:
scroll down.
CogAgent:
tap [[0.434, 0.190]].
User:
What's the price of the Galaxy
phone on eBay?
Ground Truth:
scroll right.
Figure 4. Instances in the AITW dataset where CogAgent provides
accurate responses that differ from the annotated ground truth.
5. Samples of Pre-train Data
Samples of pre-train data are listed in this section, including
data for text recognition (Fig. 5), visual grounding (Fig. 6),
and webpage-html pairs (Fig. 7).
Figure 5. Samples of pre-train data for text recognition.
14
Figure 6. Samples of pre-train data for visual grounding. Figure 7. Samples of webpage-html pairs.
15
6. Details of Fine-Tuning Data
6.1. Human annotation
To enable CogAgent to function as an agent on various apps
and websites, we assembled a team of over ten annotators
to collect and label screenshots. To ensure the quality of
the annotations and prevent overly simplistic, homogenized
content, the annotation process was divided into two phases.
In the first phase, annotators were required to independently select from a pool of apps and websites, and capture
screenshots of pages strongly relevant to the main function
of that app/website, with no less than 20 screenshots for
each app/website. Subsequently, for each screenshot, the
annotators would label them according to the following aspects:
1. Five buttons. List the names of five clickable buttons
from the screenshots. The types of buttons should be
diverse.
2. Three clickable areas. List three clickable areas from
the screenshots, e.g. text boxes, search boxes, clickable images, hyperlinks, etc.
3. Two questions extracting information from the image. Ask questions involving the textual information
in the images. For instance, “In what year did the user
in the image register?” .
4. One operation requirement. What function of the
webpage/app would you use in this interface? For example, adding the comment: “cool!”. The requirement
shouldn’t be vague such as “adding comment”.
In the second phase, annotators are required to provide grounding annotation for the questions and operational
methods proposed in the first phase (the aforementioned
parts 3 and 4). For example, for Figure 8, a possible annotation would be:
1. Buttons: Back; Search; Subscribed; Home; Library.
2. Clickable areas: Avatar in the middle top; Video preview in the middle bottom; Personal profile.
3. Question 1: Based on the page, how many followers
does this author have?
Answer: According to the personal profile at the top
[[013,568,802,188]], this author has 4.97M followers.
Question 2: Based on the page, how many videos has
this author posted?
Answer: According to the personal profile at the
top [[013,568,802,188]], this author has posted 502
videos.
Figure 8. Samples of human-collected screenshot.
4. Operation requirement: Based on the page, write out
the steps to complete the following function: Follow
this author.
Answer: Click on Subscribed [[049,826,728,078]] to
follow this author.
6.2. Conversion of Agent Datasets
To convert Mind2Web to natural language with GPT4, we
use the following prompt:
Imagine that you are a robot operating a computer.
Like how humans operate the computer, you can move
the mouse, click with the mouse, or type some texts
with the keyboard.
**Your ultimate task is: “Find the lowest-priced
round trip flight with hotel on May 2 from Kathmandu,
Nepal KTM to Shanghai, China PVG and return on
May 5. Book a double room and check out with the
default flights.”.**
16
You are given previous actions: (format: element →
operation)
1. [link] Flight + Hotel → CLICK,
2. [textbox] Where from? → TYPE: KATHMANDU,
3. [div] Tribhuvan Intl Airport (KTM), Nepal →
CLICK,
4. [textbox] Where to? → TYPE: SHANGHAI,
5. [div] Pudong Intl Airport (PVG), China →
CLICK,
6. [span] Sat 8 Apr - Sat 15 Apr → CLICK,
7. [checkbox] 2 May 2023 → CLICK,
8. [checkbox] 5 May 2023 → CLICK,
Furthermore, you **secretly** know the following
steps of action. **(But you have to pretend this is the
result of your derivation and you don’t know it before)**
1. [button] Search → CLICK,
2. [button] Recommended → CLICK,
3. [div] Lowest Price → CLICK,
4. [svg] → CLICK,
5. [button] Choose your room → CLICK,
6. [button] Book Business Double Room A →
CLICK,
7. [svg] → CLICK,
8. [svg] → CLICK,
9. [button] Choose → CLICK
Please formulate an operational guide for future operations for solving the task. The guide includes:
1. Plan: A **multi-step future** plan that does
not involve low-level operations **(start from current screen and action, DON’T include previous actions)**; steps indexed by numbers. **Be sure to pretend that you don’t know the future interface and actions, and don’t know the elements not existing on the
current screen.**
2. Action: The specific **immediate** action
that needs to be taken (Corresponding to the** very
next action** [button] Search → CLICK at the box
{“x left”: 0.876, “y left”: 0.308, “width”: 0.063,
“height”: 0.034}), which should be detailed enough to
directly control the operation, such as specifying the
exact location (or box) to click, the exact content of
the text to type, the item to select.
**Output Format:** A JSON dictionary strictly
following the format:
{ “plan”: “...<Your Plan Here>”, “action”:
“...<Your Action Here>”, “operation”: “[button]
Search → CLICK at the box “x left”: 0.876, “y left”:
0.308, “width”: 0.063, “height”: 0.034”} **Please do
not output any content other than the JSON format.
Don’t mention elements that only appear in HTML
such as svg, div.**
The returned answer from GPT-4 would be:
{ “plan”: “1. After searching, you’ll see a list of flight
and hotel packages. 2. Review the recommended options, usually highlighted for convenience. 3. Focus
on finding the lowest priced option. You might need to
navigate through various sorting or filtering features
to do this. 4. Once you’ve found the lowest priced option, select it to view more details. 5. In the detailed
view, you’ll have to choose your room type. Look for
options that fit your requirements, like a business double room. 6. After selecting the room, proceed to finalize your booking. This might involve confirming
your choice and entering any required information. 7.
Review all the details carefully before finalizing the
booking to ensure everything is correct.”,
“action”: “Click the ’Search’ button to proceed
with the search based on the entered criteria. This button is located towards the right side of the screen.”,
“operation”: “[button] Search → CLICK at the
box {“x left”: 0.876, “y left”: 0.308, “width”: 0.063,
“height”: 0.03}” }
7. More Generated Samples of CogAgent
To further demonstrate the capabilities of CogAgent, we
provide additional generated samples in this section.