Vision Mamba: Efficient Visual Representation Learning with Bidirectional
State Space Model

Figure 1. Performance and efficiency comparisons between DeiT [60] and our Vim model. For the accuracy comparison, we first pretrain
DeiT and Vim on IN1K classification dataset [10], then we finetune the generic backbones on different downstream dense prediction
tasks, i.e., semantic segmentation, object detection, instance segmentation. Results show that the proposed Vim outperforms DeiT on
both pretraining and finetuning tasks. For the speed comparison, the proposed Vim is significantly faster than DeiT when dealing with
large images because of its subquadratic-time computation. For the GPU memory comparison, Vim requires less GPU memory than DeiT
for inferring large images by its linear memory complexity. With not only superior accuracy for vision tasks, the proposed Vim is also
more efficient than DeiT in dealing with large images. For example, Vim is 2.8× faster than DeiT and saves 86.8% GPU memory when
performing batch inference to extract features on images with a resolution of 1248×1248, i.e., 6084 tokens per image




Abstract
Recently the state space models (SSMs) with efficient
hardware-aware designs, i.e., Mamba, have shown great
potential for long sequence modeling. Building efficient and
generic vision backbones purely upon SSMs is an appealing
direction. However, representing visual data is challenging
for SSMs due to the position-sensitivity of visual data and
the requirement of global context for visual understanding.
In this paper, we show that the reliance of visual representation learning on self-attention is not necessary and propose
a new generic vision backbone with bidirectional Mamba
blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet
classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance
compared to well-established vision transformers like Dei


while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8× faster
than DeiT and saves 86.8% GPU memory when performing
batch inference to extract features on images with a resolution of 1248×1248. The results demonstrate that Vim
is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for
high-resolution images and it has great potential to become
the next-generation backbone for vision foundation models.
1. Introduction
Recent research advancements have led to a surge of interest in the state space model (SSM). Originating from
the classic state space model [30], modern SSMs excel at
capturing long-range dependencies and benefit from parallel training. Some SSM-based methods, such as the linear
state-space layers (LSSL) [22], structured state space sequence model (S4) [21], diagonal state space (DSS) [24


and S4D [23], are proposed to process sequence data across
a wide range of tasks and modalities, particularly on modeling long-range dependencies. They are efficient on long
sequences because of convolutional computation and nearlinear computation. 2-D SSM [2], SGConvNeXt [37],
and ConvSSM [52] combine SSM with CNN or Transformer architecture to process 2-D data. The recent work,
Mamba [20], incorporates time-varying parameters into the
SSM and proposes a hardware-aware algorithm to enable
efficient training and inference. The superior scaling performance of Mamba indicates that it is a promising alternative to Transformer in language modeling. Nevertheless,
a generic pure-SSM-based backbone has not been explored
for vision tasks.
Vision Transformers (ViTs) have achieved great success
in visual representation learning, excelling in both largescale self-supervised pre-training and high performance on
downstream tasks. Compared with convolutional neural
networks, the core advantage lies in that ViT can provide
each image patch with data/patch-dependent global context
through self-attention. This differs from convolutional networks that use the same parameters, i.e., the convolutional
filters, for all positions. Another advantage is the modalityagnostic modeling by treating an image as a sequence of
patches without 2D inductive bias, which makes it the preferred architecture for multimodal applications [3, 36, 40].
At the same time, the self-attention mechanism in Transformers poses challenges in terms of speed and memory usage when dealing with long-range visual dependencies, e.g.,
processing high-resolution images.
Motivated by the success of Mamba in language modeling, it is appealing that we can also transfer this success
from language to vision, i.e., to design a generic and efficient visual backbone with the advanced SSM method.
However, there are two challenges for Mamba, i.e., unidirectional modeling and lack of positional awareness. To
address these challenges, we propose the Vision Mamba
(Vim) block, which incorporates the bidirectional SSMs for
data-dependent global visual context modeling and position
embeddings for location-aware visual recognition. We first
split the input image into patches and linearly project them
as vectors to Vim. Image patches are treated as the sequence data in Vim blocks, which efficiently compresses
the visual representation with the proposed bidirectional selective state space. Furthermore, the position embedding
in Vim block provides the awareness for spatial information, which enables Vim to be more robust in dense prediction tasks. In the current stage, we train the Vim model on
the supervised image classification task using the ImageNet
dataset and then use the pretrained Vim as the backbone to
perform sequential visual representation learning for downstream dense prediction tasks, i.e., semantic segmentation,
object detection, and instance segmentation. Like Transformers, Vim can be pretrained on large-scale unsupervised
visual data for better visual representation. Thanks to the
better efficiency of Mamba, the large-scale pretraining of
Vim can be achieved with lower computational cost.
Compared with other SSM-based models for vision
tasks, Vim is a pure-SSM-based method and models images in a sequence manner, which is more promising for a
generic and efficient backbone. Thanks to the bidirectional
compressing modeling with positional awareness, Vim is
the first pure-SSM-based model to handle dense prediction
tasks. Compared with the most convincing Transformerbased model, i.e., DeiT [60], Vim achieves superior performance on ImageNet classification. Furthermore, Vim
is more efficient in terms of GPU memory and inference
time for high-resolution images. The efficiency in terms of
memory and speed empowers Vim to directly perform sequential visual representation learning without relying on
2D priors (such as the 2D local window in ViTDet [38]) for
high-resolution visual understanding tasks while achieving
higher accuracy than DeiT.
Our main contributions can be summarized as follows:
• We propose Vision Mamba (Vim), which incorporates
bidirectional SSM for data-dependent global visual context modeling and position embeddings for locationaware visual understanding.
• Without the need of attention, the proposed Vim has
the same modeling power as ViT while it only has
subquadratic-time computation and linear memory complexity. Specifically, our Vim is 2.8× faster than DeiT
and saves 86.8% GPU memory when performing batch
inference to extract features on images at the resolution
of 1248×1248.
• We conduct extensive experiments on ImageNet classification and dense prediction downstream tasks. The results demonstrate that Vim achieves superior performance
compared to the well-established and highly-optimized
plain vision Transformer, i.e., DeiT.
• Benefiting from the efficient hardware-aware design of
Mamba, Vim is much more efficient than the selfattention-based DeiT [60] for high-resolution computer
vision tasks, e.g., video segmentation, aerial image analysis, medical image segmentation, computational pathology.
2. Related Work
Architectures for generic vision backbone. In the early
eras, ConvNet [34] serves as the de-facto standard network
design for computer vision. Many convolutional neural architectures [25, 26, 33, 50, 51, 56–58, 63, 72] have been
proposed as the vision backbone for various visual applications. The pioneering work, Vision Transformer (ViT) [14]
changes the landscape. It treats an image as a sequence of
flattened 2D patches and directly applies a pure Transformer


architecture. The surprising results of ViT on image classification and its scaling ability encourage a lot of follow-up
works [16, 59, 61, 62]. One line of works focuses on hybrid architecture designs by introducing 2D convolutional
priors into ViT [9, 13, 15, 69]. PVT [66] proposes a pyramid structure Transformer. Swin Transformer [42] applies
self-attention within shift windows. Another line of works
focuses on improving traditional 2D ConvNets with more
advanced settings [41, 67]. ConvNeXt [43] reviews the design space and proposes pure ConvNets, which can be scalable as ViT and its variants. RepLKNet [12] proposes to
scale up the kernel size of existing ConvNets to bring improvements.
Though these dominant follow-up works demonstrate
superior performance and better efficiency on ImageNet [10] and various downstream tasks [39, 74] by introducing 2D priors, with the surge of large-scale visual
pretraining [1, 5, 17] and multi-modality applications [3,
29, 35, 36, 40, 49], vanilla Transformer-style model strikes
back to the center stage of computer vision. The advantages
of larger modeling capacity, unified multi-modality representation, being friendly to self-supervised learning etc.,
make it the preferred architecture. However, the number
of visual tokens is limited due to the quadratic complexity
of Transformer. There are plenty of works [7, 8, 11, 32, 48,
55, 65] to address this long-standing and prominent challenge, but few of them focus on visual applications. Recently, LongViT [68] built an efficient Transformer architecture for computational pathology applications via dilated
attention. The linear computation complexity of LongViT
allows it to encode the extremely long visual sequence. In
this work, we draw inspiration from Mamba [20] and explore building a pure-SSM-based model as a generic vision
backbone without using attention, while preserving the sequential, modality-agnostic modeling merit of ViT.
State space models for long sequence modeling. [21]
proposes a Structured State-Space Sequence (S4) model, a
novel alternative to CNNs or Transformers, to model the
long-range dependency. The promising property of linearly
scaling in sequence length attracts further explorations. [53]
proposes a new S5 layer by introducing MIMO SSM and
efficient parallel scan into S4 layer. [18] designs a new
SSM layer, H3, that nearly fills the performance gap between SSMs and Transformer attention in language modeling. [46] builds the Gated State Space layer on S4 by
introducing more gating units to improve the expressivity.
Recently, [20] proposes a data-dependent SSM layer and
builds a generic language model backbone, Mamba, which
outperforms Transformers at various sizes on large-scale
real data and enjoys linear scaling in sequence length. In
this work, we explore transferring the success of Mamba to
vision, i.e., building a generic vision backbone purely upon
SSM without attention.
State space models for visual applications. [27] uses
1D S4 to handle the long-range temporal dependencies for
video classification. [47] further extends 1D S4 to handle multi-dimensional data including 2D images and 3D
videos. [28] combines the strengths of S4 and self-attention
to build TranS4mer model, achieving state-of-the-art performance for movie scene detection. [64] introduces a novel
selectivity mechanism to S4, largely improving the performance of S4 on long-form video understanding with a much
lower memory footprint. [73] supplants attention mechanisms with a more scalable SSM-based backbone to generate high-resolution images and process fine-grained representation under affordable computation. [45] proposes
U-Mamba, a hybrid CNN-SSM architecture, to handle the
long-range dependencies in biomedical image segmentation. The above works either apply SSM to specific visual
applications or build a hybrid architecture by combining
SSM with convolution or attention. Different from them,
we build a pure-SSM-based model, which can be adopted
as a generic vision backbone.
3. Method
The goal of Vision Mamba (Vim) is to introduce the advanced state space model (SSM), i.e., Mamba [20], to computer vision. This section begins with a description of the
preliminaries of SSM. It is followed by an overview of Vim.
We then detail how the Vim block processes input token sequences and proceed to illustrate the architecture details of
Vim. The section concludes with an analysis of the efficiency of the proposed Vim.
3.1. Preliminaries
The SSM-based models, i.e., structured state space sequence models (S4) and Mamba are inspired by the continuous system, which maps a 1-D function or sequence
x(t) ∈ R 7→ y(t) ∈ R through a hidden state h(t) ∈ R
N
.
This system uses A ∈ R
N×N
as the evolution parameter and
B ∈ R
N×1
, C ∈ R
1×N
as the projection parameters.
h
′
(t) = Ah(t) + Bx(t),
y(t) = Ch(t).
(1)
The S4 and Mamba are the discrete versions of the continuous system, which include a timescale parameter ∆ to
transform the continuous parameters A, B to discrete parameters A, B. The commonly used method for transformation is zero-order hold (ZOH), which is defined as follows:
A = exp (∆A),
B = (∆A)
−1
(exp (∆A) − I) · ∆B.
(2)

After the discretization of A, B, the discretized version
of Eq. (1) using a step size ∆ can be rewritten as:
ht = Aht−1 + Bxt,
yt = Cht.
(3)
At last, the models compute output through a global convolution.
K = (CB, CAB, . . . , CA
M−1
B),
y = x ∗ K,
(4)
where M is the length of the input sequence x, and K ∈ R
M
is a structured convolutional kernel.
3.2. Vision Mamba
An overview of the proposed Vim is shown in Fig. 2. The
standard Mamba is designed for the 1-D sequence. To
process the vision tasks, we first transform the 2-D image
t ∈ R
H×W×C
into the flattened 2-D patches xp ∈ R
J×(P
2
·C)
,
where (H, W) is the size of input image, C is the number of
channels, P is the size of image patches. Next, we linearly
project the xp to the vector with size D and add position
embeddings Epos ∈ R
(J+1)×D
, as follows:
T0 = [tcls; t
1
pW; t
2
pW; · · · ; t
J
pW] + Epos, (5)
where t
j
p is the j-th patch of t, W ∈ R
(P
2
·C)×D
is the
learnable projection matrix. Inspired by ViT [14] and
BERT [31], we also use class token to represent the whole
patch sequence, which is denoted as tcls. We then send
the token sequence (Tl−1) to the l-th layer of the Vim encoder, and get the output Tl. Finally, we normalize the output class token T0
L
and feed it to the multi-layer perceptron
(MLP) head to get the final prediction pˆ, as follows:
Tl = Vim(Tl−1) + Tl−1,
f = Norm(T
0
L
),
pˆ = MLP(f),
(6)
where Vim is the proposed vision mamba block, L is the
number of layers, and Norm is the normalization layer.
3.3. Vim Block
The original Mamba block is designed for the 1-D sequence, which is not suitable for vision tasks requiring
spatial-aware understanding. In this section, we introduce
the Vim block, which incorporates the bidirectional sequence modeling for the vision tasks. The Vim block is
shown in Fig. 2.
Specifically, we present the operations of Vim block in
Algo. 1. The input token sequence Tl−1 is first normalized by the normalization layer. Next, we linearly project
Algorithm 1: Vim Block Process
Input: token sequence Tl−1 : (B, M, D)
Output: token sequence Tl
: (B, M, D)
/* normalize the input sequence T′
l−1 */
1 T′
l−1
: (B, M, D) ← Norm(Tl−1)
2 x : (B, M, E) ← Linearx(T′
l−1
)
3 z : (B, M, E) ← Linearz
(T′
l−1
)
/* process with different direction */
4 for o in {forward, backward} do
5 x
′
o
: (B, M, E) ← SiLU(Conv1do(x))
6 Bo : (B, M, N) ← LinearB
o
(x
′
o
)
7 Co : (B, M, N) ← LinearC
o
(x
′
o
)
/* softplus ensures positive ∆o */
8 ∆o : (B, M, E) ←
log(1 + exp(Linear∆o
(x
′
o
) + Parameter∆o
))
/* shape of ParameterA
o is (E, N) */
9 Ao : (B, M, E, N) ← ∆o
NParameterA
o
10 Bo : (B, M, E, N) ← ∆o
NBo
11 yo : (B, M, E) ← SSM(Ao, Bo, Co)(x
′
o
)
12 end
/* get gated yo */
13 y
′
forward : (B, M, E) ← yforward JSiLU(z)
14 y
′
backward : (B, M, E) ← ybackward JSiLU(z)
/* residual connection */
15 Tl
: (B, M, D) ← LinearT(y
′
forward + y
′
backward) + Tl−1
16 Return: Tl
the normalized sequence to the x and z with dimension size
E. Then, we process the x from the forward and backward
directions. For each direction, we first apply the 1-D convolution to the x and get the x
′
o
. We then linearly project the
x
′
o
to the Bo, Co, ∆o, respectively. The ∆o is then used to
transform the Ao, Bo, respectively. Finally, we compute the
yforward and ybackward through the SSM. The yforward
and ybackward are then gated by the z and added together to
get the output token sequence Tl.
3.4. Architecture Details
In summary, the hyper-parameters of our architecture are
listed as follows:
L: the number of blocks,
D: the hidden state dimension,
E: expanded state dimension,
N: SSM dimension.
Following ViT [14] and DeiT [61], we first employ 16×16
kernel size projection layer to get a 1-D sequence of nonoverlapping patch embeddings. Subsequently, we directly
stack L Vim blocks. By default, we set the number of blocks
L to 24, SSM dimension N to 16. To align with the model
4
Embedded Patches
Norm
�
Forward
Conv1d
Backward
Conv1d
Forward
SSM
Backward
SSM
L×
Vision Mamba Encoder
Prediction
Input Image
Vision Mamba Encoder
Flatten and Linear Projection
* 0 1 2 3 4 5 6 7 8 9
Projection Layer MLP
Patch Tokens
Position Embed.
Class Token
0 1
*
Vision Mamba (Vim)
� Activation
Figure 2. The overview of the proposed Vim model. We first split the input image into patches, and then project them into patch tokens.
Last, we send the sequence of tokens to the proposed Vim encoder. To perform ImageNet classification, we concatenate an extra learnable
classification token to the patch token sequence. Different from Mamba for text sequence modeling, Vim encoder processes the token
sequence with both forward and backward directions.
sizes of DeiT series, we set the hidden state dimension D to
192 and expanded state dimension E to 384 for the tiny-size
variant. For the small-size variant, we set D to 384 and E to
768.
3.5. Efficiency Analysis
Traditional SSM-based methods leverage the fast Fourier
transform to boost the convolution operation as shown in
Eq. (4). For data-dependent methods, such as Mamba, the
SSM operation in Line 11 of Algo. 1 is no longer equivalent to convolution. To address this problem, Mamba and
the proposed Vim choose a modern-hardware-friendly way
to ensure efficiency. The key idea of this optimization is
to avoid the IO-bound and memory-bound of modern hardware accelerators (GPUs).
IO-Efficiency. The high bandwidth memory (HBM) and
SRAM are two important components for GPUs. Among
them, SRAM has a larger bandwidth and HBM has a bigger
memory size. The standard implementation of Vim’s SSM
operation with HBM requires the number of memory IO on
the order of O(BMEN). Inspired by Mamba, Vim first reads
in O(BME + EN) bytes of memory (∆o, Ao, Bo, Co) from
slow HBM to fast SRAM. Then, Vim gets the discrete Ao,
Bo of a size of (B, M, E, N) in SRAM. Last, Vim performs
SSM operations in SRAM and writes the output of a size of
(B, M, E) back to HBM. This method can help to reduce IOs
from O(BMEN) to O(BME + EN).
Memory-Efficiency. To avoid out-of-memory problems
and achieve lower memory usage when dealing with long
sequences, Vim chooses the same recomputation method as
Mamba. For the intermediate states of size (B, M, E, N) to
calculate the gradient, Vim recomputes them at the network
backward pass. For intermediate activations such as the output of activation functions and convolution, Vim also recomputes them to optimize the GPU memory requirement,
as the activation values take a lot of memory but are fast for
recomputation.
Computation-Efficiency. SSM in Vim block (Line 11 in
Algo.1) and self-attention in Transformer both play a key
role in providing global context adaptively. Given a visual
sequence T ∈ R1×M×D
and the default setting E = 2D, the
computation complexity of a global self-attention and SSM
are:
Ω(self-attention) = 4MD2 + 2M
2
D, (7)
Ω(SSM) = 3M(2D)N + M(2D)N
2
, (8)
where self-attention is quadratic to sequence length M, and
SSM is linear to sequence length M (N is a fixed parameter,
set to 16 by default). The computational efficiency makes
Vim scalable for gigapixel applications with large sequence
lengths.
4. Experiment
4.1. Image Classification
Settings. We benchmark Vim on the ImageNet-1K
dataset [10], which contains 1.28M training images and
50K validation images from 1,000 categories. All models are trained on the training set, and top-1 accuracy on
the validation set is reported. For fair comparisons, our
training settings mainly follow DeiT [61]. Specifically, we
apply random cropping, random horizontal flipping, labelsmoothing regularization, mixup, and random erasing as
data augmentations. When training on 2242
input images,
we employ AdamW [44] with a momentum of 0.9, a total
batch size of 1024, and a weight decay of 0.05 to optimize
models. We train the Vim models for 300 epochs using a
cosine schedule, 1×10−3
initial learning rate, and EMA.
During testing, we apply a center crop on the validation set
to crop out 2242
images. Experiments are performed on 8
A800 GPUs.
5
Method image
size #param. ImageNet
top-1 acc.
Convnets
ResNet-18 [25] 2242 12M 69.8
ResNet-50 [25] 2242 25M 76.2
ResNet-101 [25] 2242 45M 77.4
ResNet-152 [25] 2242 60M 78.3
ResNeXt50-32x4d [72] 2242 25M 77.6
RegNetY-4GF [50] 2242 21M 80.0
Transformers
ViT-B/16 [14] 3842 86M 77.9
ViT-L/16 [14] 3842 307M 76.5
DeiT-Ti [61] 2242 6M 72.2
DeiT-S [61] 2242 22M 79.8
SSMs
S4ND-ViT-B [47] 2242 89M 80.4
Vim-Ti 2242 7M 73.1
Vim-S 2242 26M 80.3
Table 1. Comparison with different backbones on ImageNet-1K
validation set.
Method Backbone image
size #param. val
mIoU
DeepLab v3+ [6] ResNet-101 5122
63M 44.1
UperNet [71] ResNet-50 5122
67M 41.2
UperNet [71] ResNet-101 5122
86M 44.9
UperNet [71] DeiT-Ti 5122
11M 39.2
UperNet [71] DeiT-S 5122
43M 44.0
UperNet [71] Vim-Ti 5122
13M 40.2
UperNet [71] Vim-S 5122
46M 44.9
Table 2. Results of semantic segmentation on the ADE20K val
set.
Results. Tab. 1 compares Vim with ConvNet-based,
Transformer-based and SSM-based backbones. Compared
to ConvNet-based ResNet [25], Vim demonstrates superior performance. For example, when the parameters
are roughly similar, the top-1 accuracy of Vim-Small
reaches 80.3, which is 4.1 points higher than that of
ResNet50. Compared with the conventional self-attentionbased ViT [14], Vim outperforms it by considerable margins
in terms of both parameter numbers and classification accuracy. When compared to the highly-optimized ViT-variant,
i.e., DeiT [61], Vim surpasses it at different scales with
comparable parameter numbers: 0.9 points higher for VimTiny over DeiT-Tiny, and 0.5 points higher for Vim-Small
over DeiT-Small. Compared with SSM-based S4ND-ViTB [47], Vim achieves similar top-1 accuracy with 3× fewer
parameters.
Fig. 1 (b) and (c) compare the FPS and GPU memory
of tiny-size Vim and DeiT. Vim demonstrates better efficiency in speed and memory as image resolution grows.
Specifically, when the image size is 512, Vim achieves similar FPS and memory as DeiT. As the image size grows to
1248, Vim is 2.8× faster than DeiT and saves 86.8% GPU
memory. The pronounced superiority of Vim’s linear scaling in sequence length makes it ready for high-resolution
downstream vision applications and long-sequence multimodality applications.
4.2. Semantic Segmentation
Settings. We conduct experiments for semantic segmentation on the ADE20K [74] dataset. ADE20K contains 150
fine-grained semantic categories, with 20K, 2K, and 3K images for training, validation, and testing, respectively. We
choose UperNet [70] as our base framework. In training,
we employ AdamW with a weight decay of 0.01, and a total
batch size of 16 to optimize models. The employed training schedule uses an initial learning rate of 6×10−5
, linear
learning rate decay, a linear warmup of 1, 500 iterations,
and a total training of 160K iterations. The data augmentations follow common settings, including random horizontal
flipping, random re-scaling within the ratio range [0.5, 2.0],
and random photometric distortion. During evaluation, we
rescale the image to have a shorter side of 512.
Results. As shown in Tab. 2, Vim consistently outperforms DeiT across different scales: 1.0 mIoU higher for
Vim-Ti over DeiT-Ti, and 0.9 mIoU higher for Vim-S over
DeiT-S. Compared to the ResNet-101 backbone, our VimS achieves the same segmentation performance with nearly
2× fewer parameters.
To further evaluate the efficiency for downstream tasks,
i.e., segmentation, detection, and instance segmentation, we
combine the backbones with a commonly used feature pyramid network (FPN) module and benchmark their FPS and
GPU memory. As shown in Fig. 3 and Fig. 4, the efficiency
curves demonstrate similar comparison results of the pure
backbone (Fig. 1), though we append a heavy FPN on the
backbones. The exceptional linear scaling performance is
attributed to our proposed efficient backbone Vim, which
builds the foundation for learning gigapixel-level visual representation in an end-to-end manner without the need for
multi-stage encoding (e.g., aerial image, medical image,
and computational pathology).
4.3. Object Detection and Instance Segmentation
Settings. We conduct experiments for object detection
and instance segmentation on the COCO 2017 dataset [39].
6
1
1.4
1.8
2.2
2.6
512 640 738 1024 1248
FPS w/ log scale
Resolution
DeiT Vim
2.52
2.24
2.00
1.56
1.25
2.27
2.06
1.90
1.70
Faster
2.8
× faster
Figure 3. FPS comparison between DeiT-Ti [60] and our VimTi on the commonly used downstream framework. We perform
batch inference and benchmark the log-scaled FPS on the architecture with the backbone and FPN. Vim achieves comparable performance to DeiT with a small resolution, i.e., 512. As the input
image resolution increases, Vim will have a higher FPS.
Backbone APbox APbox
50 APbox
75 APbox
s APbox
m APbox
l
DeiT-Ti 44.4 63.0 47.8 26.1 47.4 61.8
Vim-Ti 45.7 63.9 49.6 26.1 49.0 63.2
Backbone APmask APmask
50 APmask
75 APmask
s APmask
m APmask
l
DeiT-Ti 38.1 59.9 40.5 18.1 40.5 58.4
Vim-Ti 39.2 60.9 41.7 18.2 41.8 60.2
Table 3. Results of object detection and instance segmentation on
the COCO val set using Cascade Mask R-CNN [4] framework.
The COCO 2017 dataset contains 118K images for training, 5K images for validating, and 20K images for testing.
We use the canonical Cascade Mask R-CNN [4] as the base
framework. For ViT-based backbones, we apply extra configurations (e.g., interleaved window & global attention) to
handle the high-resolution images following ViTDet [38].
For SSM-based Vim, we directly use it without any modifications. Other training and evaluation settings are just the
same. During training, we employ AdamW with a weight
decay of 0.1, and a total batch size of 64 to optimize models.
The employed training schedule uses an initial learning rate
of 1×10−4
, linear learning rate decay, and a total training
of 380K iterations. The data augmentations use large-scale
jitter data augmentation [19] to 1024×1024 input images.
During evaluation, we rescale the image to have a shorter
side of 1024.
Results. Tab. 3 compares Vim-Ti with DeiT-Ti using Cascade Mask R-CNN framework [4]. Vim-Ti surpasses DeiTTi by 1.3 box AP and 1.1 mask AP. For the middle-size
and large-size objects, Vim-Ti outperforms DeiT-Ti by 1.6
APbox
m /1.3 APmask
m and 1.4 APbox
l
/1.8 APmask
l
, demonstrating
better long-range context learning than DeiT (Fig. 5).
5
20
35
50
65
80
512 640 738 1024 1248
GPU Memory (GB)
Resolution
DeiT Vim
Smaller
-73.2% memory
OOM
5.04 6.88 8.54
15.86
22.59
5.52 8.09 12.48
40.03
Figure 4. GPU memory efficiency comparison between DeiTTi [60] and our Vim-Ti on the commonly used downstream framework. We perform batch inference and benchmark the GPU memory on the architecture with the backbone and FPN. Vim requires
comparable GPU memory to DeiT with a small resolution, i.e.,
512. As the input image resolution increases, Vim will use significantly less GPU memory.
Bidirectional strategy ImageNet
top-1 acc.
ADE20K
mIoU
None 73.2 32.3
Bidirectional Layer 70.9 33.6
Bidirectional SSM 72.8 33.2
Bidirectional SSM + Conv1d 73.1 34.8
Table 4. Ablation study on the bidirectional design. Default setting
for Vim is marked in blue .
We want to highlight that the accuracy superiority is nontrivial since DeiT is equipped with window attention while
Vim works in a pure sequence modeling manner. Specifically, to perform representation learning on high-resolution
images (i.e., 1024×1024), we follow ViTDet [38] and modify DeiT backbone with the use of 2D window attention,
which injects 2D prior and breaks the sequential modeling
nature of Transformer. Thanks to the efficiency illustrated
in Sec. 3.5, Fig. 1 and Fig. 4, we can directly apply Vim on
1024×1024 input images and learn sequential visual representation for object detection and instance segmentation
without need for 2D priors in the backbone.
4.4. Ablation Study
We ablate the key bidirectional design of Vim, using ImageNet-1K classification and Segmenter [54] on
ADE20K semantic segmentation. To fully evaluate the
power of learned representation on ImageNet, we use a simple Segmenter head with only 2 layers to perform transfer
learning on semantic segmentation. We study these bidirectional strategies:
• None. We directly adopt Mamba block to process visual
sequence with only the forward direction.
7
GT DeiT-Ti Vim-Ti
Figure 5. Visualization comparison of DeiT-Ti [61] and our Vim-Ti on the Cascade Mask R-CNN [4] framework. Thanks to the long-range
context learning of SSM, we can capture the very large object in the image, which the DeiT-Ti counterpart fails to perceive.
• Bidirectional Sequence. During training, we randomly
flip the visual sequence. This works like data augmentation.
• Bidirectional Block. We pair the stacked blocks. The
first block of each pair processes visual sequence in the
forward direction, and the second block of each pair processes in the backward direction.
• Bidirectional SSM. We add an extra SSM for each block
to process the visual sequence in the backward direction.
• Bidirectional SSM + Conv1d. Based on Bidirectional
SSM, we further add a backward Conv1d before the backward SSM (Fig. 2).
As shown in Tab. 4, directly adopting Mamba block
achieves good performance in classification. However, the
unnatural unidirectional manner poses challenges in downstream dense prediction. Specifically, the preliminary bidirectional strategy of using Bidirectional Block achieves 7
points lower top-1 accuracy on classification. Yet, it outperforms the vanilla unidirectional Mamba block by 1.3
mIoU on semantic segmentation. By adding extra backward SSM and Conv1d, we achieve similar classification
accuracy (73.1 top-1 acc vs. 73.2 top-1 acc) and exceptional
segmentation superiority (34.8 mIoU vs. 32.3 mIoU). We
use the strategy of Bidirectional SSM + Conv1d as the default setting in our Vim block.
5. Conclusion and Future Work
We have proposed Vision Mamba (Vim) to explore the
very recent efficient state space model, i.e., Mamba, as
generic vision backbones. Unlike prior state space models for vision tasks which use hybrid architecture or equivalent global 2D convolutional kernel, Vim learns visual representation in the sequence modeling manner and does not
introduce image-specific inductive biases. Thanks to the
proposed bidirectional state space modeling, Vim achieves
data-dependent global visual context and enjoys the same
modeling power as Transformer, while having lower computation complexity. Benefiting from the hardware-aware
designs of Mamba, the inference speed and memory usage of Vim are significantly better than ViTs when processing high-resolution images. Experiment results on standard computer vision benchmarks have verified the modeling power and high efficiency of Vim, showing that Vim has
great potential to be the next-generation vision backbone.
In future works, Vim with the bidirectional SSM modeling with position embeddings is suitable for unsupervised
tasks such as mask image modeling pretraining and the
similar architecture with Mamba enables multimodal tasks
such as CLIP-style pretraining. Based on the pretrained
Vim weights, exploring the usefulness of Vim for analyzing high-resolution medical images, remote sensing images,
and long videos, which can be regarded as downstream
tasks, is very straightforward.
Acknowledgement
We would like to acknowledge Tianheng Cheng, Yuxin
Fang, Shusheng Yang, Bo Jiang, and Jingfeng Yao for their
helpful feedback on the draft.
