VisualWebArena: EVALUATING MULTIMODAL
AGENTS ON REALISTIC VISUAL WEB TASKS
Jing Yu Koh Robert Lo∗ Lawrence Jang∗ Vikram Duvvur∗
Ming Chong Lim∗ Po-Yu Huang∗ Graham Neubig Shuyan Zhou
Ruslan Salakhutdinov Daniel Fried
Carnegie Mellon University
{jingyuk,rsalakhu,dfried}@cs.cmu.edu
ABSTRACT
Autonomous agents capable of planning, reasoning, and executing actions on the
web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting
many natural tasks that require visual information to effectively solve. Given
that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic visually
grounded tasks. VisualWebArena comprises of a set of diverse and complex webbased tasks that evaluate various capabilities of autonomous multimodal agents.
To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to
accomplish user-defined objectives. We conduct an extensive evaluation of stateof-the-art LLM-based autonomous agents, including several multimodal models.
Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-ofthe-art multimodal language agents. VisualWebArena provides a framework for
evaluating multimodal autonomous language agents, and offers insights towards
building stronger autonomous agents for the web. Our code, baseline models, and
data is publicly available at https://jykoh.com/vwa.
1 INTRODUCTION
Automating routine computer tasks with autonomous agents is a long standing goal of artificial intelligence research (Franklin & Graesser, 1996; Jennings et al., 1998). To achieve this, we need
agents that can navigate computers effectively, process visual and textual inputs, handle high-level
natural language instructions, and execute actions to achieve desired goals. As digital interfaces today are primarily built for human eyes, effective visual understanding is necessary for many routine
computer tasks. For example, humans frequently perform tasks on the web which involve visual
references, such as “Help me order a green polo shirt from Amazon”. Many real world tasks also
involve understanding image content: people frequently rely on pictures to communicate rather than
text descriptions. Productive work done on the computer is also often explicitly visual, e.g., working
with Microsoft Excel sheets, creating presentations in Google Slides, or performing creative work
in Adobe Photoshop. However, many agent benchmarks today focus on text-based tasks, neglecting
the evaluation (and consequently the development) of multimodal autonomous agents.
To address this gap, we propose VisualWebArena (Fig. 1), a benchmark suite designed to rigorously
assess and advance the visual and textual capabilities of autonomous agents. VisualWebArena
builds off the WebArena (Zhou et al., 2024) framework, which comes with reproducible self-hosted
environments and execution-based evaluations. VisualWebArena introduces a set of unique visual
tasks, and emphasizes integrating visual understanding with language processing, closely simulating
human interaction with modern computing interfaces.
∗Equal contribution.
1
arXiv:2401.13649v1 [cs.LG] 24 Jan 2024
Preprint
VisualWebArena Sites
Knowledge Resources + Tools
“Buy the cheapest color photo
printer and send it to Emily's
place (as shown in the image).”
“Help me make a post selling
this item and navigate to it.
Price it at $10 cheaper than the
most similar item on the site.”
“Navigate to the comments section of the
latest image post in the /f/Art subreddit
that contains animals.”
Webpage Task Specification
LLM / VLM
Agent
click
[1602]
Figure 1: VisualWebArena is a benchmark suite of realistic classifieds, e-commerce, and Reddit
sites. We benchmark state-of-the-art LLM and VLM agents on 910 diverse, natural, and visually
grounded tasks that involve web navigation, action execution, and visual understanding.
By introducing VisualWebArena, we aim to bridge the existing gap in the evaluation of multimodal
autonomous agents and to push the boundaries of AI-driven task automation. We envision a future
where AI agents can not only process textual information but also effectively navigate and interact
with visual settings (e.g., digital screens), truly augmenting human productivity and creativity. Our
contributions are summarized as follows:
• We introduce VisualWebArena, a set of 910 realistic tasks over three diverse web environments: Classifieds, Shopping, and Reddit. The Classifieds environment is a new contribution with real world data, while the Shopping and Reddit environments are inherited from
WebArena. All tasks we introduce are visually grounded, and require visual understanding
of webpage content to effectively solve (while WebArena does not). 25.2% of our tasks also
take images as input (Fig. 1), and require understanding of interleaved image-text inputs to
correctly process and attempt. Our code, tasks, and baselines is publicly released.1
• We extensively benchmark the autonomous capabilities of state-of-the-art (SOTA) large
language models (LLM) and vision-language models (VLMs), demonstrating that strong
VLMs outperform text-based LLMs. The best VLM agents achieve a success rate of only
16.4% on VisualWebArena tasks, significantly below human performance of 88.7%. Our
results also highlight a large gap between API-based and open sourced VLM agents.
• We propose a new VLM agent inspired by Set-of-Marks prompting Yang et al. (2023a),
simplifying the action space of the model. We show that this model substantially outperforms LLM agents, especially on sites that are more visually complex.
2 RELATED WORK
Language-Guided Web Agent Benchmarks The development of reproducible environments for
autonomous agents has seen considerable progress in recent years. Earlier efforts introduced reinforcement learning environments (Brockman et al., 2016), but extended into the web (Shi et al.,
2017; Liu et al., 2018), which laid the groundwork for reproducible web-based environments. Recent benchmarks in this domain introduced tasks involving actions on static internet pages (Deng
et al., 2023) as well as interaction in simulated web environments (Yao et al., 2022; Zhou et al.,
2024). AgentBench (Liu et al., 2023c) also extends the scope of agents for computer interaction
beyond the web, exploring database management and operating system functionalities.
LLM Agents There has been significant recent interest in using Large Language Models
(LLMs) for developing autonomous agents (Xi et al., 2023; Wang et al., 2023a). State-of-the-art
LLMs (Google, 2023; OpenAI, 2023; Chowdhery et al., 2023; Rae et al., 2021; Zhang et al., 2022;
Touvron et al., 2023a;b; Jiang et al., 2023; 2024) based on the Transformer (Vaswani et al., 2017)
architecture have demonstrated impressive abilities in learning from in-context examples (Brown
et al., 2020; Chan et al., 2022), reasoning (Wei et al., 2022; Yao et al., 2023; Wang et al., 2023c;
1
https://jykoh.com/vwa
2
Preprint
Besta et al., 2023), following instructions (Chung et al., 2022; Longpre et al., 2023; Ouyang et al.,
2022), and operating over long-context sequences (Tay et al., 2021; Bertsch et al., 2023; Tworkowski
et al., 2023). Several recent works leverage these compelling abilities for building autonomous web
agents: Kim et al. (2023) propose a recursive prompting method to improve the performance of
GPT-4 on the MiniWoB++ (Liu et al., 2018) benchmark. Liu et al. (2023d) propose a method of
orchestrating multiple LLM agents to improve performance on web navigation in the WebShop (Yao
et al., 2022) environment. Zeng et al. (2023) fine-tunes the LLaMA-2 (Touvron et al., 2023b) models
on a set of interaction trajectories with instructions, improving over baseline agent models.
Vision-Language Models Finally, our work builds off advances in vision-language models
(VLMs) which can process image-text inputs to generate text, used for many multimodal tasks
such as image captioning (Vinyals et al., 2015), visual question answering (Antol et al., 2015), and
other benchmarks (Mialon et al., 2023; Yue et al., 2023; Tong et al., 2024). Frozen (Tsimpoukelli
et al., 2021) was one of the first approaches to demonstrate the effectiveness of finetuning a visual
encoder to map images into the embedding space of a LLM, introducing compelling few-shot multimodal abilities. Alayrac et al. (2022) introduced cross-attention layers and scaled up models and
training data to improve multimodal performance. Wang et al. (2023b) introduced trainable visual
expert modules to improve the fusion between the vision and language models. Liu et al. (2023b)
proposed fine-tuning on images paired with instructions to improve text generation performance
on several multimodal tasks. GPT-4V (OpenAI, 2023) introduces visual processing abilities to the
GPT-4 models, unlocking many compelling abilities (Yang et al., 2023c;a). Gemini (Google, 2023)
is multimodal from the beginning (in contrast to post-hoc fine-tuned models), and can handle text
input interleaved with visual and audio inputs. Several recent work have also explored the use of
strong VLMs for building visual agents for mobile platforms (Zhan & Zhang, 2023; Chu et al., 2023;
Yang et al., 2023b) as well as for the web (Gur et al., 2023; Hong et al., 2023). (Zheng et al., 2024)
is contemporaneous work which performs action grounding to identify appropriate HTML elements
for enabling VLM agents to execute actions on the web. In contrast, our proposed SoM agent directly leverages JavaScript to produce a Set-of-Marks (Yang et al., 2023a) for the VLM agent to
directly use as both an action and observation space.
3 VisualWebArena ENVIRONMENT
In order to ensure reproducibility, realism, and determinism, we ensure that all websites in the
VisualWebArena framework are available as standalone open-source web applications. The textual
and visual content available in the websites are acquired from real world counterparts, while the
code is based off open-source infrastructure commonly used in real world applications. We formally
define the environment, observation space, and action space in the following sections, but encourage
readers to refer to WebArena (Zhou et al., 2024) for more details.
The environment and agent can be modeled as a partially observable Markov decision process
(POMDP): E = (S, A, Ω, T), where S represents the set of states, A represents the set of actions
(Sec. 3.2), and Ω represents the set of observations (Sec. 3.1). The transition function is defined as
T : S × A → S, with deterministic transitions between states conditioned on actions. At each time
step t, the environment is in some state st (e.g., a particular page), with a partial observation ot ∈ Ω.
An agent issues an action at ∈ A conditioned on ot, which results in a new state st+1 ∈ S and a new
partial observation ot+1 ∈ Ω of the resulting page. The action at may be an action to be executed on
the webpage (Tab. 1), or it may simply be a string output for information seeking tasks (Sec. 3.3).
Finally, we define the reward function R : S×A → {0, 1} (Sec. 3.3) to measure the success of a task
execution. In VisualWebArena, the reward function returns 1 at the final step if the state transitions
align with the expectations of the task objective (i.e., the goal is achieved), and 0 otherwise. For
example, in the first task in Fig. 1, the reward function evaluates whether the order was correctly
placed to the exact address provided in the input image, and contains the correct item.
3.1 OBSERVATION SPACE
The observation space Ω is modeled after a realistic web browsing experience. Observations include
the webpage URLs, opened tabs (possibly multiple tabs of different websites), and the webpage
content of the focused tab. In approximately 25% of tasks, the intent also involves one or more input
3
Preprint
“I'm trying to find
this post. Navigate
to the comment
section for it.”
Original Webpage
Webpage with SoM of Interactable Elements
SoM Elements and TextContent
LLM / VLM
Agent
click [31]
...
[7] [A] [Comments]
[8] [BUTTON] [Hot]
[9] [IMG] [description: picture of a pumpkin]
[10] [A] [kneechalice]
[11] [A] [45 comments]
...
Figure 2: Set-of-Marks (Yang et al., 2023a) augmented webpage screenshot. Every interactable
element is highlighted with a bounding box and a unique ID.
images which are provided as part of the observation (e.g., the first and third tasks in Fig. 1). The
webpage content can be represented in several different ways:
1. Raw web page HTML as a Document Object Model (DOM) tree, commonly used in previous work on autonomous web agents (Shi et al., 2017; Liu et al., 2018; Deng et al., 2023).
2. Web page screenshots, represented as RGB arrays, which has demonstrated efficacy in prior
work on visual agents (Gur et al., 2023; Hong et al., 2023; Yan et al., 2023).
3. The accessibility tree,2 which provides a structured and simplified representation of the
webpage content that is optimized for assistive technologies. This is the primary representation that WebArena (Zhou et al., 2024) uses for its baseline LLM agents.
4. We introduce a new visual representation inspired by Set-of-Marks (SoM) prompting (Yang
et al., 2023a). For every interactable element on the webpage, we label it with a bounding
box and an ID (Fig. 2), producing a screenshot that allows a visual agent to reference
elements on the page by their unique ID. We provide more details and analysis in Sec. 5.3.
3.2 ACTION SPACE
Action Type a Description
click [elem] Click on element elem.
hover [elem] Hover on element elem.
type [elem] [text] Type text on element elem.
press [key comb] Press a key combination.
new tab Open a new tab.
tab focus [index] Focus on the i-th tab.
tab close Close current tab.
goto [url] Open url.
go back Click the back button.
go forward Click the forward button.
scroll [up|down] Scroll up or down the page.
stop [answer] End the task with an optional output.
Table 1: Set of possible actions A.
The full set of actions A is summarized in Tab. 1. The
arguments for action at is the unique element ID from
the current observation ot. An advantage of this representation (over predicting (x, y) coordinates) is that it
allows us to focus on high level reasoning rather than
low-level control, as many SOTA VLMs and LLMs
were not explicitly trained for referencing elements at
such fine granularity. For the agents with accessibility tree representations, the argument is the element
ID in the tree. For the SoM representation, we use the
unique IDs assigned in the current page (see Fig. 2).
3.3 EVALUATION
In order to evaluate performance on VisualWebArena, we introduce new visually grounded evaluation metrics to the functional evaluation paradigm of WebArena. These allow us to comprehensively
evaluate the correctness of execution traces on open ended visually grounded tasks. The rewards for
each task are hand designed functions using the primitives described below.
2
https://developer.mozilla.org/en-US/docs/Glossary/Accessibility tree
4
Preprint
Webpage / Input Image(s) Example Intent Reward Function r(s, a) Implementation
What is the ISIN of the company
that occupies the largest portion
in Warren Buffet’s portfolio? Answer using the information from
the Wikipedia site in the second
tab.
exact match(aˆ, "US0378331005")
Add something like what the man
is wearing to my wish list.
url="/wishlist"
locator(".wishlist .product-image-photo")
eval vqa(s, "Is this a polo shirt? (yes/no)", "yes")
eval vqa(s, "Is this shirt green? (yes/no)", "yes")
Create a post for each of the following images in the most related
forums.
eval fuzzy image match(s, a
∗)
Navigate to my listing of the white
car and change the price to $25000.
Update the price in the description
as well.
url="/index.php?page=item&id=84144"
must include(aˆ, "$25000 |OR| $25,000")
must exclude(aˆ, "$30000 |OR| $30,000")
Table 2: Various evaluation metrics to assign reward r(s, a) ∈ R : S × A → {0, 1}. Our executionbased reward primitives allow us to benchmark many diverse, realistic, and open-ended tasks.
Information Seeking Tasks Information seeking tasks (e.g., the first task in Tab. 2) expect a
string output aˆ from the model. We mostly adopt similar reward functions introduced in WebArena
for measuring text correctness against a groundtruth output a
∗
:
• exact match: This can be defined as 1{aˆ=a∗} (where 1 represents the indicator function).
Only outputs that are exactly equal to the groundtruth are given a score of 1. This is used
in tasks where an exact response (e.g., a numerical answer) is expected.
• must include: This reward function gives a score of 1 if all elements in a
∗
are contained in aˆ and 0 otherwise. For example, if aˆ = “$1.99, $2.50, $10.00” and a
∗ =
{“1.99”, “2.50”, “10.00”}, the task is awarded a score of 1 as all expected elements are
present in the output. This is primarily used in tasks where we expect an unordered list of
outputs, or we expect text output to contain a particular keyword.
• must exclude: This is a new function we introduce, which is the converse of
must include. A reward of 0 is assigned if any element from a specified set a
∗
is found in aˆ
(and 1 otherwise). For instance, if aˆ = “$1.99, $2.50, $10.00” and a
∗ = {“1.50”, “2.00”},
the reward is 1 as none of the prohibited elements are in the output.
• fuzzy match: This function queries a LLM (in our implementation,
gpt-4-1106-preview, to evaluate whether a
∗
and aˆ are semantically equivalent.
The LLM is prompted to output either “correct”, “incorrect”, or “partially correct”, and we
assign a reward of 1 if the output is “correct” and 0 otherwise.3
In addition, we also introduce several new visual functions for measuring open ended tasks:
• eval vqa: Similar to fuzzy match, this function queries a VLM capable of performing
visual question answering (VQA) (Antol et al., 2015). We use BLIP-2-T5XL Li et al.
(2023) in our implementation. We query the VLM with an image and a question. If the
output of the VLM contains the groundtruth answer a
∗
, a reward of 1 is assigned (and 0
otherwise). This is useful for evaluating more open ended tasks, e.g., “Buy me a green
hoodie under $10.”. There are many possible products that satisfy this objective, and it
would be infeasible to enumerate all their IDs.
• eval fuzzy image match: This function checks whether a query image is similar to a
groundtruth image according to the structural similarity index measure (SSIM) (Wang et al.,
3We do not consider non-binary rewards in this work, but it would be a useful direction to explore in the
future towards more continuous scales of performance.
5
Preprint
Shopping
50.9%
Reddit
18.7%
Classifieds
25.5%
Multi-Site
4.9%
Distribution of Tasks Across Sites
Sites
Shopping Reddit Classifieds Multi-Site
Figure 3: Tasks proportion by sites.
Easy Medium Hard
Visual Difficulty
Easy Medium Hard
Action Difficulty
15.7% 12.9% 4.2%
13.7% 12.6% 8.5%
7.0% 10.5% 14.8%
Distribution of Tasks by Difficulty
Figure 4: Tasks proportion by difficulty.
2004). If the SSIM between a query image and the groundtruth image is higher than a
prespecified threshold t ∈ [0, 1], a reward of 1 is assigned, and 0 otherwise.
Navigation and Actions Many tasks in VisualWebArena require navigating through multiple
webpages, and executing actions to change the underlying state s of the environment. To accurately
evaluate certain objectives, we require reward functions that examine the final webpage state to determine whether the task was successfully accomplished. These adopt the WebArena programmatic
evaluation metrics, as well as the eval vqa and eval fuzzy image match metrics for measuring
performance on visual or open-ended tasks.
Each evaluator consists of a locator as well as a URL. The URL can be a specific page, or a function
(e.g., the last page that the agent navigated to). The locator describes the object on the page that
should be examined (e.g., all img elements, or all elements with the .product-image-photo class).
During evaluation, we use the locator to retrieve the corresponding image or text content, and reuse
the functions from the information seeking tasks to check for correctness.
4 CURATING VISUALLY GROUNDED TASKS
4.1 WEB ENVIRONMENTS
VisualWebArena is designed around three realistic web environments that involve visually rich
content. Several tasks also require referencing information from a self-hosted Wikipedia knowledge
base, and others involve interaction across more than one of these websites (Fig. 3).
Classifieds We introduce a new Classifieds website in VisualWebArena, inspired by real world
marketplaces such as Craigslist and Facebook Marketplace. This new site provides a distinct environment compared to existing ones in WebArena, introducing visually grounded tasks centered
around user interactions typical in online classifieds websites (posting, searching, commenting).
The site’s infrastructure uses OSClass,4
a robust open-source Content Management System (CMS)
designed for classifieds websites, used in multiple real world sites. OSClass enables us to simulate
functions such as search, posting new listings, commenting, and leaving reviews and ratings. The
site contains 65,955 listings, each consisting of a title, a description, and a product image.
Shopping The Shopping site follows the e-commerce environment from WebArena (Zhou et al.,
2024), with product information and content scraped from Amazon and released in WebShop (Yao
et al., 2022). Visual understanding of product images is required for successfully navigating and
completing tasks on e-commerce platforms, making this a natural choice for VisualWebArena. Most
products on the site include at least one image, and the platform covers a diverse range of products.
4
https://osclass-classifieds.com/
6
Preprint
Reddit The Reddit site also follows the same environment from WebArena, and represents a social
forum platform. The site contains 31,464 posts containing a diverse set of images across different
subreddits and forums, such as natural images, memes, consumer electronics, and charts. The inclusion of the Reddit environment allows us to create a comprehensive benchmark that evaluates an
agent’s ability to understand and operate with different types of visual information.
4.2 TASKS
We introduce a set of 910 new tasks, split across the three sites detailed earlier (Fig. 3). These
tasks necessitate visual comprehension, and are designed to assess the visual and reasoning skills of
autonomous agents in web-based environments.
Task Creation We focus on curating realistic visually grounded tasks, following a similar process as task creation in WebArena. We start by curating intent templates (e.g., “Find me the
{{attribute}} {{item}}. It should be between {{range}}.”), which can be manually expanded
by the annotator with different arguments to form multiple tasks (e.g., “Find me the cheapest red
Toyota. It should be between $3000 to $6000.”). The tasks were curated by 6 graduate students (coauthors of this paper). We encouraged annotators to be creative, and make use of the visual layouts
of the websites, input images, and cross-site functionalities to develop creative and realistic tasks.
When tasks involved input images, these were sourced from royalty-free, attribution-free sources,
and MS-COCO (Lin et al., 2014). Annotators also wrote the reward functions using the primitives
described in Sec. 3.3. We collected a total of 314 templates (average of 2.9 tasks per template).
While the majority of tasks can be solved, we also included a small subset (46 tasks, or 5.1%) which
are unachievable. This subset tests the ability of agents to terminate early in the event where a task
cannot be solved, which is essential in many real world scenarios.
Visually Grounded Tasks A key aspect of VisualWebArena is the inherent visual grounding of
all tasks. Each task demands visual understanding, requiring agents to process and interpret visual
information rather than relying solely on textual or HTML-based cues. This aligns closely with
modern human-computer interfaces, where visual information (e.g., icons, colors) is often critical.
For instance, a typical task might involve selecting a visually specific item, such as a “green polo
shirt” where the color is visually discernible but not explicitly mentioned in text.
Task Complexity We classify each task into three difficulty levels: easy, medium, and hard. This
classification is particularly useful for assessing performance across a spectrum of agents, ranging
from smaller models to state-of-the-art LLMs and VLMs. We find in our analysis (Sec. 5) that many
open-source models (e.g., LLaMA-2-70B, IDEFICS-80B) achieve a success rate of close to 0 on
medium or hard tasks, but non-zero performance on easy tasks. This suggests that running opensource models on the easy subset would provide useful signal during development as well as faster
iteration cycles (assuming performance between weaker and stronger agents are correlated).
We annotate both the action and visual difficulty of each task (breakdown in Fig. 4). The action
difficulty is determined by the estimated number of actions that a human would need to complete
the task. Easy tasks are defined as those that require three or fewer actions, Medium tasks involve
four to nine actions, and Hard tasks demand ten or more.
Visual difficulty is similarly segmented, with each difficulty level reflecting the complexity of visual
processing required: Easy tasks involve basic visual identification such as colors, shapes, and highlevel object detection (e.g., recognizing the existence of a cat). Medium tasks require discerning
patterns, semantic understanding, and OCR on text of shorter lengths or large fonts. Hard tasks
involve multiple image inputs, OCR on small or lengthy text, or detection of fine details.
4.3 HUMAN PERFORMANCE
We measure the success rate of 7 college students (who are familiar with commercial versions of
the sites) on VisualWebArena tasks. Several of these students also assisted with task creation, and
to avoid data leakage, we ensured that they were not assigned to the same tasks that they initially
created. We sample one task per template, collecting a representative set of 230 tasks. We find that
humans do well at this task, achieving an overall success rate of 88.7% (Tab. 3). The mistakes made
7
Preprint
in the remaining 11.3% of tasks are usually minor, such as not reading the task correctly or missing
a part of the objective. Another common failure mode was for tasks that required exhaustive search
(e.g., “Navigate to the comments section of this exact image.”). Users were often unable to find the
appropriate post after searching for 5-10 mins and gave up, assuming that the task was unachievable.
These types of laborious tasks represent problems that strong computer agents would be well poised
to solve, possibly achieving above human performance (and speed).
5 BASELINES
We run several baselines on VisualWebArena to benchmark the performance of state-of-the-art
LLM and VLM agents. We use a webpage viewport size of 1280 × 2048, and truncate text observations to 3840 tokens (or 15360 characters for Gemini). For models with shorter context windows
(e.g., LLaMA, IDEFICS, CogVLM), we instead use a viewport size of 1280 × 720 and truncate text
observations to 640 tokens. For GPT-3.5 and GPT-4 models, we follow Zhou et al. (2024) in using a
temperature of 1.0 and a top-p of 0.9. For Gemini models we use the suggested default temperature
of 0.9 and top-p of 1.0. For the remaining models, we find that they benefit from sampling from
lower temperatures, and use a temperature of 0.6 and top-p of 0.95. Nucleus sampling (Holtzman
et al., 2020) is used in all experiments. All models are prompt-based and provided with 3 in-context
examples (one from each environment), which share no overlap with the benchmark tasks. The
prompts we use are provided in the appendix. We summarize the results in Tab. 3 and describe the
baselines in detail in the following sections.
5.1 TEXT-BASED LLM AGENTS
Several prior works have developed strong autonomous agents through prompting text-only
LLMs (Zhou et al., 2024; Kim et al., 2023; Liu et al., 2023d). We run several text-based LLM agents
on the accessibility tree representations of the websites, by prompting them with Chain-of-Thought
prompting (Wei et al., 2022), similar to Zhou et al. (2024). We leave more advanced prompting
strategies for future work. Our baselines include several API-based LLM models, including GPT4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), Gemini-Pro, as well as
open sourced LLMs such as LLaMA-2-70B and Mixtral-8x7B.
5.2 IMAGE CAPTION AUGMENTED LLM AGENTS
VisualWebArena is a visually grounded benchmark, and we expect that leveraging complementary
visual information would improve performance. Hence, we run pretrained image captioning models
on every img element on the HTML page, and augment the accessibility tree with this information as
the image alt-text before passing this as input to the LLM agents. If a task contains input images, we
also caption them and include the captions as part of the prompt. We run experiments on GPT-3.5
with two recent image captioning models, BLIP-2-T5XL (Li et al., 2023) and LLaVA-v1.5-7B (Liu
et al., 2023a). Our results with GPT-3.5 as the LLM backbone (“Caption-augmented” section of
Tab. 3) suggest that the LLaVA and BLIP-2 captioning models achieve comparable performance.
Since BLIP-2 achieves a slightly higher success rate, is a smaller model, and requires less GPU
VRAM, we use it as the captioning backbone for the remaining experiments.
5.3 MULTIMODAL AGENTS
Finally, we benchmark the capabilities of strong closed-source and open-source vision language
models. Unlike the previous text-based models, these multimodal models are trained on large
datasets of paired text and images, allowing them to learn joint representations between vision and
language. We evaluate several models capable of processing multiple interleaved image-and-text
inputs: GPT-4V (OpenAI, 2023), Gemini-Pro (Google, 2023), IDEFICS-80B-Instruct5
(an opensource reproduction of Flamingo (Alayrac et al., 2022)), and CogVLM (Wang et al., 2023b). We
experiment with two different input formats for these models:
5
https://huggingface.co/HuggingFaceM4/idefics-80b-instruct
8
Preprint
Model Type LLM Backbone Visual Backbone Inputs Success Rate (↑)
Classifieds Reddit Shopping Overall
Text-only
LLaMA-2-70B
- Acc. Tree
0.43% 1.43% 1.29% 1.10%
Mixtral-8x7B 1.71% 2.86% 1.29% 1.76%
Gemini-Pro 0.85% 0.95% 3.43% 2.20%
GPT-3.5 0.43% 0.95% 3.65% 2.20%
GPT-4 5.56% 4.76% 9.23% 7.25%
Caption-augmented
LLaMA-2-70B BLIP-2-T5XL
Acc. Tree + Caps
0.00% 0.95% 0.86% 0.66%
Mixtral-8x7B BLIP-2-T5XL 1.28% 0.48% 2.79% 1.87%
GPT-3.5 LLaVA-7B 1.28% 1.43% 4.08% 2.75%
GPT-3.5 BLIP-2-T5XL 0.85% 1.43% 4.72% 2.97%
Gemini-Pro BLIP-2-T5XL 1.71% 1.43% 6.01% 3.85%
GPT-4 BLIP-2-T5XL 8.55% 8.57% 16.74% 12.75%
Multimodal
IDEFICS-80B-Instruct
Image + Caps + Acc. Tree
0.43% 0.95% 0.86% 0.77%
CogVLM 0.00% 0.48% 0.43% 0.33%
Gemini-Pro 3.42% 4.29% 8.15% 6.04%
GPT-4V 8.12% 12.38% 19.74% 15.05%
Multimodal (SoM)
IDEFICS-80B-Instruct
Image + Caps + SoM
0.85% 0.95% 1.07% 0.99%
CogVLM 0.00% 0.48% 0.43% 0.33%
Gemini-Pro 3.42% 3.81% 7.73% 5.71%
GPT-4V 9.83% 17.14% 19.31% 16.37%
Human Performance - - Webpage 91.07% 87.10% 88.39% 88.70%
Table 3: Success rates of baseline LLM and VLM agents on VisualWebArena.
Image Screenshot + Captions + Accessibility Tree: This approach provides the accessibility
tree representation augmented with image captions as accessibility tree alt-text from BLIP-2-T5XL
(similar to the caption-augmented agent), as well as the screenshot of the current webpage as inputs.
Image Screenshot + Captions + SoM: Inspired by Set-of-Marks prompting Yang et al. (2023a),
we perform an initial preprocessing step by using JavaScript to automatically annotate every interactable element on the webpage with a bounding box and a unique ID. The annotated screenshot
containing bounding boxes and IDs, are provided as input to the multimodal model along with a text
representation of the SoM (see Fig. 2). Similar to the baselines above, we also provide the captions
from BLIP-2-T5XL for all img elements on the page. There have been several projects6
that propose
similar representations. Most have been proof-of-concept demos, and to the best of our knowledge,
we are the first to systematically benchmark this on a realistic and interactive web environment.
5.4 RESULTS
Our main baseline results are summarized in Tab. 3. All existing models significantly underperform
compared to humans, which indicate significant headroom in VisualWebArena for future work. We
discuss some main findings below, with more detailed analysis in Sec. 6.
Text-based LLMs Perform Poorly State-of-the-art text-only LLMs generally achieve poor results, with the best model (GPT-4) achieving an overall success rate of 7.25%. When we augment
the LLMs with caption information, this considerably improves success rate, from 7.25% to 12.75%
for GPT-4. Other models also see a similar boost in success rate when provided caption information.
Multimodality Helps We achieve a substantial improvement in success rate when we use multimodal agents: GPT-4V (gpt-4-1106-vision-preview) achieves an overall success rate of 15.05%,
substantially improving over the text-only GPT-4 model. Gemini-Pro also experiences a significant
uplift in success rate, improving from 3.85% (caption-augmented) to 6.04% (multimodal). Text-only
or caption-augmented models may be limited in their ability to process complex images (e.g., those
that require OCR or recognition of non-salient objects), thus falling behind multimodal models.
SoM Improves Navigability We observe that the SoM representation (Sec. 5.3) further improves
the performance of GPT-4V over using the accessibility tree observation and action space, boosting
overall success rate (15.05% → 16.37%). We observe particularly substantial improvements compared to the accessibility tree GPT-4V model on Classifieds and Reddit, from 12.38% → 17.14%
6
For example, GPT-4V-ACT and vimGPT propose similar interfaces.
9
Preprint
Visual Difficulty (v)
a \v Easy Medium Hard Overall a \v Easy Medium Hard Overall
Easy 18.9% 11.1% 10.5% 14.8% Easy 30.1% 20.5% 26.3% 25.8%
Medium 1.6% 6.1% 7.8% 4.7% Medium 15.2% 11.3% 11.7% 12.9%
hard 1.6% 4.2% 1.5% 2.4% hard 14.1% 10.4% 8.9% 10.5%
Overall 9.0% 7.3% 4.8% 7.3% Overall 21.4% 14.3% 12.4% 16.4%
(a) Success rate of GPT-4 Text-only (c) Success rate of GPT-4V + SoM
a \v Easy Medium Hard Overall a \v Easy Medium Hard Overall
Easy 23.1% 18.8% 13.2% 20.1% Easy 6.0 7.7 6.1 6.9
Medium 14.4% 9.6% 5.2% 10.4% Medium 10.4 10.6 7.2 10.0
Hard 7.8% 7.3% 8.1% 7.8% Hard 14.1 9.2 12.5 12.1
Overall 16.9% 12.2% 8.0% 12.7% Overall 9.5 9.4 10.2 9.6
Action Difficulty (a)
(b) Success rate of GPT-4 + Captions (d) Trajectory length of GPT-4V + SoM
Figure 5: Success rates (a, b, c) and trajectory lengths (d) across different difficulty levels.
and 8.12% → 9.83% respectively. We attribute this to the Classifieds and Reddit websites containing more dense visual content. For example, these websites often contain many smaller sized images
that are arranged very closely (Fig. 2). In many of these cases, the accessibility tree does not always
provide sufficient information to disentangle elements that are spatially close. We hypothesize that
the SoM representation is superior in these cases, as a strong VLM model is able to more accurately
disentangle and click on the desired content. For the other VLMs, SoM does not significantly improve success rates, which we attribute to the finding from Yang et al. (2023a) that only GPT-4V
demonstrates this emergent grounding ability (perhaps due to scale or training data). This motivates
future work in imbueing VLM agents with similar abilities.
6 ANALYSIS
6.1 PERFORMANCE BY TASK DIFFICULTY
We conduct an analysis of the GPT-4 models across different action and visual difficulty levels
(Fig. 5). We observe that success rate generally decreases as action/vision difficulty increases, which
makes intuitive sense based on the difficulty taxonomy described in Sec. 4.2. The findings also
show that multimodal models perform better especially on hard visual tasks. On this subset, GPT4V + SoM achieves an average success rate of 12.4%, which is significantly higher than that of the
caption-augmented (8.0%) and the text-only agents (4.8%).
In addition to success rates, we also observed that the GPT-4V trajectory lengths increased with
action difficulty, with harder tasks requiring more steps to complete.
6.2 PERFORMANCE BY TASK TYPE
We also analyze the success rate of the best VLM agent baseline (GPT-4V with SoM prompting)
across several additional subsets of tasks. The results are summarized in Tab. 4 and discussed in the
following paragraphs, with further analysis for other models provided in the appendix.
Task Subset % of Total SR (↑)
OCR required 17.1% 13.4%
No OCR required 82.9% 16.9%
Exact image match 8.7% 18.9%
No exact image match 91.3% 16.2%
Image inputs 25.2% 19.0%
No image inputs 74.8% 14.9%
Table 4: Success rate (SR) of GPT-4V
(SoM) across different types of tasks.
OCR Tasks 17.1% of VisualWebArena require optical character recognition (OCR), such as reading text
from product images, or extracting text from an input
image. We find that GPT-4V + SoM generally performs worse on tasks that require OCR (success rate
of 13.4%) compared to tasks which do not (success
rate of 16.9%), suggesting that OCR capabilities may
be a bottleneck for completing certain tasks.
Exact Image Match 8.7% of tasks require exact image matching, which evaluates visual capabilities beyond semantic image recognition, and requires agents to identify precise visual matches. GPT-4V +
SoM achieves a slightly improved success rate (18.9%) compared to tasks that do not require exact
match (16.2%), suggesting that exact image matching is not a hurdle for the model.
10
Preprint
 click [7]
type [3]
[/f/memes] click [1]
Step 0: Start on the homepage
of Reddit
Step 1: Search "/f/memes" Step 2: Navigate to the list of all
forums
click [20]
Step 3: Navigate to the
/f/memes forum
Step 4: Go to the prole page of
the user who posted the target
image
click [54] click [41]
Step 5: Click the block button Step 6: Conrm blocking the
user
Task: "I don't like the
author of this image
from one of the hot posts
in /f/memes. Can you
help me block them?"
Figure 6: Successful execution trajectory of the GPT-4V + SoM agent on the task for blocking a
user that posted a certain picture. The text in red represents the commands output by the agent.
Image Input Tasks 25.2% of VisualWebArena include one or more input images as part of the
objective. These tasks generally appear more tractable for the GPT-4V + SoM agent, and it achieves
a higher success rate (19.0%) compared to tasks without image inputs (14.9%).
6.3 QUALITATIVE RESULTS
Successful Execution We observed that the GPT-4V + SoM agent is able to complete several
complex tasks efficiently. One example that we found particularly compelling was task 139 for
Reddit, which requires exact image matching to find a post and block a user (Fig. 6). The model
initially attempts to search for the correct forum, and when this fails it navigates to the list of forums.
After navigating correctly to /f/memes, it identifies the offending image out of the many images on
the page (Step 3 in Fig. 6) and blocks the author efficiently without any unnecessary actions.
Figure 7: The starting page
for the task “Add the red one
in the second row of this page
to my shopping cart.”
Failure Modes We observed that in several examples, the agents
would correctly perform a task but undo it, leading to failure. The
GPT-4 captioning-only model on shopping task 54 (“Add the one
[poster] with waves to my wish list.”) made an assumption that the
product image with a caption about a lighthouse was the correct
one, and added it to the wishlist. However, after going to the wish
list page the agent removes the poster because “there is no explicit
mention of waves in the current items listed on the Wish List page.”
This issue is not unique to the text input agents; even the GPT-4
SoM agent faced a similar problem in shopping task 397 (“Buy the
item on the page with a banana theme.”). The agent initially added
the correct item to the shopping cart and proceeded to check out,
but stopped in the middle stating in the reasoning trace output that
it does not think the item fit the criteria (despite having added it to
the cart just a few steps ago).
Failures on Easy Tasks We observed surprisingly poor performance on many tasks with easy action and easy visual difficulty
levels, such as in shopping task 46, which tasks the agent to add
the red product in the second row to the cart (starting on the page
shown in Fig. 7). The multimodal and SoM GPT-4V agents clicked
on a blue tablecloth in the first row and gave up when they couldn’t
find an option to order it in red. Despite appearing to be a simple
task (the correct product is the red cloth in the second row), none of the agents we benchmarked
were able to successfully complete it.
11
Preprint
Spatial Reasoning Both the multimodal and SoM agents outperform the caption and text-only
agents on tasks that require spatial reasoning. For shopping task 81 (“What is the price range for
products in the first row.”), the caption-augmented GPT-4 model makes an assumption that there
are three items to a row and provides the price range incorrectly. However, there are actually four
products in each row, which the multimodal and SoM GPT-4V agents both correctly identify to
succeed at this task.
SoM Benefits Longer Action Sequences The SoM representation generally performs better on
tasks that require more navigation steps. For example, classifieds task 31 asks the agent to “Find
the latest listing of a white Google Pixel phone and post a comment offering $10 less than their
asking price.” While the multimodal model was unable to search for the correct terms, the SoM
model was able to leverage the simplified action space to traverse more efficiently throughout the
environment. It succeeded at this task by filtering for cell phones after the initial search for more
relevant results, and managed to fill out the necessary comment form fields. We believe that the
SoM representation is superior to the multimodal representation (which only has access to the page
screenshot and accessibility tree). With SoM, the agent does not have to implicitly perform visual
co-referencing to match elements from the accessibility tree to the visual buttons and inputs that it
wants to interact with.
7 CONCLUSION
In this work, we introduced VisualWebArena, a benchmark of realistic tasks designed to rigorously
evaluate and advance the capabilities of autonomous multimodal web agents. VisualWebArena
represents a significant step towards addressing the gap in the evaluation of multimodal agents on
visually grounded tasks. We also introduce a visual agent inspired by Set-of-Marks prompting, and
demonstrate the potential of this approach for simplifying action spaces and improving performance
on visually complex websites. Our extensive evaluation of state-of-the-art LLM and VLM agents
demonstrate that while VLMs show promise, there remains a considerable performance gap compared to humans, who achieve very high success rates on VisualWebArena. Our quantitative and
qualitative analysis also highlights several common failure modes of existing LLM and VLM agents.
We expect future work on improving the reasoning, visual understanding, and planning abilities of
agents to be particularly exciting and promising areas.
ACKNOWLEDGEMENTS
We thank photographers on Pexels.com for providing free to use images. We thank Wendy Kua for
assisting with measuring human performance and for help with the figures. We thank Yi Tay, Zhiruo
Wang, Saujas Vaduguru, Paul Liang, Stephen McAleer, and many others for feedback and helpful
discussions on previous versions of the paper


APPENDIX
We provide further analysis on model failure modes for Gemini and GPT-4 (Sec. A), more details
on the new Classifieds environment (Sec. B), and on the task collection process (Sec. C).
A FURTHER ANALYSIS
A.1 FEW-SHOT PROMPTING
# Examples Success Rate (↑)
Classifieds Reddit Shopping Overall
0 4.29% 2.38% 0.43% 2.86%
1 5.36% 1.43% 2.14% 3.63%
3 8.15% 4.29% 3.42% 6.04%
Table 5: Performance with different number of in-context examples.
In most of our main experimental results, we prompt the model with 3 in-context examples. We
perform an analysis of the success rate against the number of in-context examples provided (Tab. 5).
For 1-shot experiments, we provide the model with the single in-context example from its corresponding environment. All experiments are run with the multimodal Gemini-Pro model (as GPT-4V
is prohibitively expensive) with the Image + Caption + Acc. Tree as the observation space.
We observe that overall success rate tends to increase with the number of examples provided, with
a significant jump from 1 to 3 in-context examples. The improved results with a greater number of
examples suggest that the performance of the VLM agents may improve significantly if we fine-tune
the models on web trajectories, which will be an exciting direction for future work.
A.2 TASKS ANALYSIS
In this section, we provide more fine-grained analysis across different task subsets, similar to the
one in Sec. 6.2 of the main paper. We examine both the GPT-4 text and multimodal agents, as well
as the Gemini-Pro agents. This analysis may provide useful insights towards capabilities that future
VLM models should have to perform well on web navigation tasks (specifically, OCR, exact image
matching, and handling multiple interleaved image and text inputs).
OCR Tasks On OCR tasks, which take up 17.1% of the benchmark, we observe that the GPT-4
family of models achieve a lower success rate on tasks that require OCR compared to tasks that
do not (Fig. 8). This is consistent with the findings for GPT-4V + SoM reported in Sec. 6.2 of
the main paper. We also observe that introducing multimodality (over just captions) substantially
improves performance on OCR tasks (from 6.4% to 12.2%), showcasing the importance of having
multimodal models for text recognition capabilities, as captioning models generally do not capture
such finegrained information.
For Gemini-Pro agents, we also observe similar trends, with the multimodal and SoM models achieving a higher than proportionate gain on the OCR subset (compared to the non-OCR subset). Interestingly, the multimodal Gemini-Pro agents achieve a higher success rate on tasks that require OCR
compared to tasks that do not. These results may suggest that it has strong inherent OCR capabilities,
which we believe will be useful to explore in future work (especially on the stronger Gemini-Ultra
model once it is generally available).
Exact Image Match Of the tasks in VisualWebArena, 8.7% require exact image matching, which
tests the ability of agents to identify images that have the exact same content (in contrast to those
that are just semantically similar). From Fig. 9, we observe that the GPT-4V SoM model achieves a
higher succeess rate on tasks that expect exact image match, while the other GPT-4 agents achieve
a relatively lower success rate on the exact match subset. This suggests that the SoM representation
may be more optimal for exact image match, due to its visual-centric observation and action space.
For the Gemini models, we observe that success rates on exact match tasks are substantially lower
than success rates on non-exact match tasks. Interestingly, we also observe a similar trend as the
16
Preprint
Figure 8: Success rate of GPT-4 and Gemini agents on tasks that do not require OCR vs. tasks that
do.
Figure 9: Success rates of agents on tasks that require exact image match vs. those that do not.
GPT-4 agents, where introducing multimodality improves success rates on exact match tasks, which
is further bolstered with the SoM representation.
17
Preprint
Figure 10: Success rates of agents on tasks that include input images as part of the specification vs.
tasks that are specified with just written text.
Image Input Tasks 25.2% (229 tasks) in VisualWebArena are specified with image inputs (e.g.,
the task in Fig. 6, and the first and third tasks in Fig. 1). The results of the Gemini-Pro and GPT-4
agents are summarized in Fig. 10.
We observe that for the GPT-4 agent, success rates are generally higher on tasks that involve image
inputs, with the exception of the text-only agent. This aligns with intuition, as agents that do not
have access to visual information would not be able to understand the task correctly, and would
perform worse at successfully accomplishing it. For the captioning, multimodal, and SoM GPT-4
agents, success rates are higher on the tasks involving image input, which we attribute to these tasks
being more tractable once the visual content is correctly understood.
Interestingly, we see a contrast with the Gemini-Pro agents, where success rate is generally lower on
tasks that involve input images. This may imply that the model may not be able to process multiple
interleaved image-text inputs as well. This may be useful to revisit in the future with the stronger
Gemini-Ultra model once it is released, or with stronger open sourced VLMs. We believe that being
able to handle interleaved multimodal inputs will be a core requirement for strong web agents, and
more comprehensive error analysis with stronger models may yield useful insights.
Trajectory Lengths vs. Success Rates Hard reasoning tasks, on average, require more steps to
be successfully solved. We plot the trajectory length of the GPT-4V + SoM model in Fig. 11. The
findings suggest that the model assumes a significant portion of tasks can be completed in a few
steps, as it terminates a majority of tasks after less than 10 steps. However, this assumption doesn’t
imply that the model successfully solves the majority of tasks: the error rate remains relatively
uniform across longer trajectory lengths.
A.3 OTHER FAILURE MODES
We described some failure modes in Sec. 6 of the main paper. In this section, we describe other
common issues we observed.
Giving Up Too Early Another frequent issue we observed that occurred across all the agents was
giving up too early. For example, GPT-4V + SoM fails on shopping task 248 (“Order a 6 pack of
18
Preprint
0 5 10 15 20 25 30 35
Trajectory Length
0
50
100
150
200
250
Count
PASS
FAIL
0 5 10 15 20 25 30 35
Trajectory Length
0%
20%
40%
60%
80%
100%
Success Rate
22.0%
11.4%
16.0% 14.7% 10.8%
21.1%
9.9%
78.0%
88.6%
84.0% 85.3% 89.2%
78.9%
90.1%
GPT-4V + SoM
FAIL
PASS
Figure 11: Performance of the GPT-4V + SoM agent across different trajectory lengths.
the green chocolate bars. If the shipping is more than 7% of the total price, leave a 3 star review
mentioning it, otherwise 5.”). This tasks involves several steps which the model is able to correctly
plan out, but the very first action needed is to slightly scroll down so the “add to cart” button is
visible. However, even after identifying the correct product the model gives up on the first step
instead of scrolling, because it does not immediately see the button. There are other instances of
this occurring, such as in shopping task 175, where an agent will use the search bar to search for
something, and then immediately give up because it does not see the target product instead of trying
new ways to find the product.
Getting Stuck in Loops Another issue we observed was oscillating or looping between pages,
where the agent would look something up or navigate to a page, unsuccessfully attempt to perform
the next action (such as adding it to the cart), and on failure it goes back and repeats from the
beginning. An example of this is in classifieds task 205 where the model is tasked to compare two
makeup palettes in two tabs, and the GPT-4V agent spends most of the time switching between the
tabs. We believe that these issues will likely be alleviated by introducing more sophisticated tracking
of past states and execution history, which is a promising direction for future work.
Example of a Failure Case: Changing Account Phone Number One particular task for OneStopShop and Wikipedia showcased several points of failure that we saw throughout the execution
traces for many agents. Fig. 12 contains the execution trace of GPT-4V for the task “Prepend the
country code of South Korea to the phone number of my account profile.” Below is a breakdown of
the three major mistakes in this execution.
• Useless actions In step 3 of the trajectory, the agent creates a new blank tab and does not
interact with it for the rest of the trajectory. While this does not impact the correctness of
the final task, it does show that the agents sometimes take unnecessary steps.
• Appending text instead of replacing Many agents added text to input fields without deleting the previous text, which would often result in long, repeating search queries or addresses. An example of this occurs in step 7 of Fig. 12.
• Repeating actions Another frequent issue we saw across agents was repeating actions, like
how the agent kept jumping between step 6 and step 7 of Fig. 12 until it hit the maximum
trajectory length. In this case, we believe this looping effect stems from the issue mentioned
above and each time the agent tries to correct the phone number, it keeps appending the
correct number instead of replacing the incorrect number with the correct number.
B THE CLASSIFIEDS ENVIRONMENT
The Classifieds environment contains 65,955 listings, each with a title, text description, and a product image of the item being sold. To populate the site with realistic content, we scraped data across a
variety of categories on Craigslist over 3 weeks, focusing on the Northeastern States of the US (similar to the geographic region in the Reddit site). This approach ensured a diverse and rich dataset,
19
Preprint
 click [7]
click
[1318]
click
[3917]
Step 0: Start on the homepage of
One Stop Market
Step 1: Navigate to "My Account"
page
Step 2: Press "Edit Address"
page_focus [1]
Step 4: Switch tabs to a country code
map
Step 5: Switch back to "Edit Address"
page
page_focus
[0]
Step 6: Append "+816505551212" to
the phone number and enter
Step 7: Press "Change Billing
Address"
Task: "Prepend the country
code of South Korea to the
phone number of my
account prole.
"
type [6346]
[+826505551212]
Step 3: Create new blank tab
new_tab
click
[8780]
type [6346] [+826505551212]
Tab 0: Homepage of One Stop
Market
Tab 1: Map of country codes from
Wikipedia
Figure 12: Unsuccessful execution trajectory of the GPT-4V agent on the task for adding the a
country code to the user’s phone number. The text in red represents the commands output by the
agent.
representative of real-world classifieds posts. We utilized the scrubadub Python package for redacting Personally Identifiable Information (PII), including addresses, phone numbers, and emails. We
use generated placeholders for names (e.g., “Bill Smith”), emails with fictitious addresses (e.g.,
bill smith@example.com), and phone numbers with the fictional 555-prefix numbers.
Fig. 13 and 14 show two pages within the Classifieds site, the homepage and the detail page of a
particular listing. Users can also use the search function, or filter posts by category or location to
find items.
C TASK COLLECTION PROCESS
Our main task collection process is described in Sec. 4.2. We collected the set of 910 tasks by
recruiting 6 computer science graduate students (co-authors of this paper), who were all familiar
with commercial versions of the Classifieds, Shopping, and Reddit sites, and have used them in their
personal lives.
Annotators were first instructed to spend some time exploring the VisualWebArena websites, to
familiarize themselves with their functionality and content (as this may differ slightly from real
world implementations). During task creation, we encouraged annotators to be creative, and make
use of the visual layouts of the websites, input images, and cross-site functionalities to develop
creative and realistic tasks. We ensured that there were no repeated tasks, and that there were not
too many tasks of the same type (by first producing templates, followed by instantiating them with
different arguments to create multiple tasks, as described in Sec. 4.2).
D AGENT PROMPTS
We share the system message (Fig. 15) and the prompt with in-context examples (Fig. 16) for the
baseline SoM agents. We prompt the model with 3 in-context examples for all baselines. For
multimodal and SoM models, we include the screenshot of each in-context example as well as the
screenshot of the current page. For text-only and caption augmented models, the examples consist
of just the text and captions.
20
Preprint
Figure 13: Homepage of the Classifieds site. Users can search for keywords, filter by category, or
post location.
21
Preprint
Figure 14: Example post in the Classifieds website. Users can add comments and reviews to individual listings.
22
Preprint
You are an autonomous intelligent agent tasked with navigating a web browser. You will be given
web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.
Here’s the information you’ll have:
The user’s objective: This is the task you’re trying to complete.
The current web page screenshot: This is a screenshot of the webpage, with each interactable
element assigned a unique numerical id. Each bounding box and its respective id shares the same
color.
The observation, which lists the IDs of all interactable elements on the current web page with their
text content if any, in the format [id] [tagType] [text content]. tagType is the type of the element,
such as button, link, or textbox. text content is the text content of the element. For example, [1234]
[button] [’Add to Cart’] means that there is a button with id 1234 and text content ’Add to Cart’
on the current web page. [] [StaticText] [text] means that the element is of some text that is not
interactable.
The current web page’s URL: This is the page you’re currently navigating.
The open tabs: These are the tabs you have open.
The previous action: This is the action you just performed. It may be helpful to track your progress.
The actions you can perform fall into several categories:
Page Operation Actions:
ˋˋˋclick [id]ˋˋˋ: This action clicks on an element with a specific id on the webpage.
ˋˋˋtype [id] [content]ˋˋˋ: Use this to type the content into the field with id. By default, the “Enter”
key is pressed after typing unless press enter after is set to 0, i.e., ˋˋˋtype [id] [content] [0]ˋˋˋ.
ˋˋˋhover [id]ˋˋˋ: Hover over an element with id.
ˋˋˋpress [key comb]ˋˋˋ: Simulates the pressing of a key combination on the keyboard (e.g.,
Ctrl+v).
ˋˋˋscroll [down]ˋˋˋ or ˋˋˋscroll [up]ˋˋˋ: Scroll the page up or down.
Tab Management Actions:
ˋˋˋnew tabˋˋˋ: Open a new, empty browser tab.
ˋˋˋtab focus [tab index]ˋˋˋ: Switch the browser’s focus to a specific tab using its index.
ˋˋˋclose tabˋˋˋ: Close the currently active tab.
URL Navigation Actions:
ˋˋˋgoto [url]ˋˋˋ: Navigate to a specific URL.
ˋˋˋgo backˋˋˋ: Navigate to the previously viewed page.
ˋˋˋgo forwardˋˋˋ: Navigate to the next page (if a previous ’go back’ action was performed).
Completion Action:
ˋˋˋstop [answer]ˋˋˋ: Issue this action when you believe the task is complete. If the objective is to
find a text-based answer, provide the answer in the bracket.
Homepage:
If you want to visit other websites, check out the homepage at http://homepage.com. It has a list of
websites you can visit.
http://homepage.com/password.html lists all the account name and password for the websites. You
can use them to log in to the websites.
To be successful, it is very important to follow the following rules:
1. You should only issue an action that is valid given the current observation
2. You should only issue one action at a time.
3. You should follow the examples to reason step by step and then issue the next action.
4. Generate the action in the correct format. Start with a “In summary, the next action I will perform
is” phrase, followed by action inside ˋˋˋˋˋˋ. For example, “In summary, the next action I will
perform is ˋˋˋclick [1234]ˋˋˋ”.
5. Issue stop action when you think you have achieved the objective. Don’t generate anything after
stop.
Figure 15: System message used in our SoM agent.
23
Preprint
example user:
IMAGES: (1) current page screenshot
OBSERVATION:
[31] [IMG] [Image, description: hp fx-7010dn fax machine, url: B08GKZ3ZKD.0.jpg]
[32] [A] [HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)]
[ ] [StaticText] [$279.49]
[33] [BUTTON] [Add to Cart]
[34] [A] [Add to Wish List]
[35] [A] [Add to Compare]
URL: http://onestopmarket.com/office-products/office-electronics.html
OBJECTIVE: What is the price of HP Inkjet Fax Machine?
PREVIOUS ACTION: None
example assistant: Let’s think step-by-step. This page list the information of HP Inkjet Fax Machine, which is
the product identified in the objective. Its price is $279.49. I think I have achieved the objective. I will issue
the stop action with the answer. In summary, the next action I will perform is ´´´stop [$279.49]´´´
example user:
IMAGES: (1) current page screenshot
OBSERVATION:
[ ] [StaticText] [/f/food]
[ ] [StaticText] [[homemade] Obligatory Halloween Pumpkin Loaf! Submitted by kneechalice t3 yid9lu 1
year ago]
[9] [IMG] [ ]
[ ] [StaticText] [Submitted by kneechalice t3 yid9lu 1 year ago]
[10] [A] [kneechalice]
[11] [A] [45 comments]
[ ] [StaticText] [[I ate] Maple Pecan Croissant Submitted by AccordingtoJP t3 y3hrpn 1 year ago]
[14] [IMG] [ ]
[ ] [StaticText] [Submitted by AccordingtoJP t3 y3hrpn 1 year ago]
[15] [A] [AccordingtoJP]
[16] [A] [204 comments]
URL: http://reddit.com
OBJECTIVE: Tell me what the top comment on the croissant post says.
PREVIOUS ACTION: None
example assistant: Let’s think step-by-step. This page has a post titled ’[I ate] Maple Pecan Croissant’, which
is the post mentioned in the objective. In order to find the top comment, I will navigate into the comments
section of the post. In summary, the next action I will perform is ´´´click [11]´´´
example user:
IMAGES: (1) current page screenshot
OBSERVATION:
[ ] [StaticText] [What are you looking for today?]
[5] [INPUT] [ ]
[6] [SELECT] [Select a category]
[7] [BUTTON] [Search]
[ ] [StaticText] [Latest Listings]
[ ] [StaticText] [Atlas Powered Audio System w/ Tripod 150.00 $ Music instruments Borough of Red Lion
(Pennsylvania) 2023/11/16]
[8] [IMG] [Atlas Powered Audio System w/ Tripod]
[9] [A] [Atlas Powered Audio System w/ Tripod]
[ ] [StaticText] [150.00 $]
[ ] [StaticText] [Neptune Gaming Console 350.00 $ Video gaming Pennwyn (Pennsylvania) 2023/11/16]
[10] [IMG] [Neptune Gaming Console]
[11] [A] [Neptune Gaming Console]
[ ] [StaticText] [350.00 $]
URL: http://classifieds.com
OBJECTIVE: Help me find the cheapest dark colored guitar.
PREVIOUS ACTION: None
example assistant: Let’s think step-by-step. The objective is to find the cheapest dark colored guitar on the
site. The site has a search box whose ID is [5]. I can search for guitars by entering “guitar”. I can submit this
by pressing the Enter afterwards. In summary, the next action I will perform is ´´´type [5] [guitar] [1]´´´
Figure 16: In-context examples and prompt used for our SoM agents. The webpage screenshots
with SoM are added to the end of each round of the example user dialogue.